#    -*- mode: org -*-


Archived entries from file /home/fredericb/udem/nlp/project/readme.org


* easier/pop links
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-01-30 Sat 21:21
  :ARCHIVE_FILE: ~/udem/nlp/project/readme.org
  :ARCHIVE_OLPATH: ressources
  :ARCHIVE_CATEGORY: readme
  :END:

- positional encoding
- [[https://kazemnejad.com/blog/transformer_architecture_positional_encoding/][Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's ...]]

- attention
  + [[https://towardsdatascience.com/attention-models-in-nlp-a-quick-introduction-2593c1fe35eb][Attention models in NLP a quick introduction | by Manish Chablani | Towards D...]]
  + [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
  + [[https://theaisummer.com/attention/][How Attention works in Deep Learning: understanding the attention mechanism i...]]

- videos
  + [[https://www.youtube.com/watch?v=S27pHKBEp30][LSTM is dead. Long Live Transformers! - YouTube]]

* Convolutional Sequence To Sequence Learning
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-01-30 Sat 22:13
  :ARCHIVE_FILE: ~/udem/nlp/project/readme.org
  :ARCHIVE_OLPATH: ressources/arxiv links
  :ARCHIVE_CATEGORY: readme
  :END:
- [[https://arxiv.org/abs/1705.03122][arxiv Convolutional sequence to sequence learning]] :
- file:./papers/convolutionalsequencetosequencelearning.pdf

*This paper contains Positional Encoding which used by transformer*
*Two papers are extensively referenced: (Bahdanau et al., 2014; Luong et al., 2015)*

"The dominant approach to date encodes the input sequence with a se- ries of
bi-directional recurrent neural networks (RNN) and generates a variable length
output with another set of de- coder RNNs, both of which interface via a
*soft-attention mechanism* (Bahdanau et al., 2014; Luong et al., 2015)."

"Compared to recurrent layers, convolutions create representations for fixed
size contexts, however, the effective context size of the network can easily
be made larger by stacking several layers on top of each other. This allows to
precisely control the maximum length of dependencies to be modeled."

"Convolutional networks do not depend on the computations of the previous time
step and therefore allow parallelization over every element in a sequence.
This contrasts with RNNs which maintain a hidden state of the entire past that
prevents parallel computation within a sequence."

"Multi-layer convolutional neural networks create hierarchi- cal representations
over the input sequence in which nearby input elements interact at lower layers
while distant ele- ments interact at higher layers. Hierarchical structure pro-
vides a shorter path to capture long-range dependencies compared to the chain
structure modeled by recurrent net- works,"

``In this paper we propose an architecture for sequence to se-
quence modeling that is entirely convolutional. Our model
is equipped with gated linear units (Dauphin et al., 2016)
and residual connections (He et al., 2015a). We also use
attention in every decoder layer and demonstrate that each
attention layer only adds a negligible amount of overhead.
The combination of these choices enables us to tackle large
scale problems''

``The encoder RNN processes an input sequence x =
$(x_1 , \ldots , x_m )$ of m elements and returns state representa-
tions $z = (z_1, \ldots , z_m )$. The decoder RNN takes z and
generates the output sequence $y = (y 1 , \ldots , y_n )$ left to
right, one element at a time. To generate output $y_{i+1}$ , the
decoder computes a new hidden state $h_{i+1}$ based on the
previous state $h_i$ , an embedding $g_i$ of the previous target
language word $y_i$ , as well as a conditional input $c_i$ derived
from the encoder output z."

``Models without attention consider only the final encoder
state z m by setting c i = z m for all i (Cho et al., 2014), or
simply initialize the first decoder state with z m (Sutskever
et al., 2014), in which case c i is not used. Architectures
with attention *(Bahdanau et al., 2014; Luong et al., 2015)*
compute c i as a weighted sum of (z 1 . . . . , z m ) at each time
step. The weights of the sum are referred to as attention
scores and allow the network to focus on different parts of
the input sequence as it generates the output sequences. At-
tention scores are computed by essentially comparing each
encoder state z j to a combination of the previous decoder
state h i and the last prediction y i ; the result is normalized
to be a distribution over input elements."


* TODO understand how CNN's seem to replace RNN for sequences
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-01-30 Sat 22:14
  :ARCHIVE_FILE: ~/udem/nlp/project/readme.org
  :ARCHIVE_OLPATH: steps/project proposal
  :ARCHIVE_CATEGORY: readme
  :ARCHIVE_TODO: TODO
  :END:

* DONE create master bibtex file
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-01-30 Sat 22:14
  :ARCHIVE_FILE: ~/udem/nlp/project/readme.org
  :ARCHIVE_OLPATH: steps/project proposal
  :ARCHIVE_CATEGORY: readme
  :ARCHIVE_TODO: DONE
  :END:

* DONE project proposal latex template
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-01-30 Sat 22:14
  :ARCHIVE_FILE: ~/udem/nlp/project/readme.org
  :ARCHIVE_OLPATH: steps/project proposal
  :ARCHIVE_CATEGORY: readme
  :ARCHIVE_TODO: DONE
  :END:

* Neural machine translation by jointly learning to align and translate
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-05 Fri 14:29
:ARCHIVE_FILE: ~/udem/nlp/project/readme.org
:ARCHIVE_OLPATH: transformers
:ARCHIVE_CATEGORY: readme
:END:
*note: best first paper to read*
** intro

#+begin_quote
In this paper, we conjecture that the use of a fixed-length vector is a
bottleneck in improving the performance of this basic encoder–decoder architec-
ture, and propose to extend this by allowing a model to automatically
(soft-)search for parts of a source sentence that are relevant to predicting a
target word, without having to form these parts as a hard segment explicitly.
#+end_quote

#+begin_quote
Cho et al. (2014b) showed that indeed the performance of a basic
encoder–decoder deteriorates rapidly as the length of an input sentence
increases.
#+end_quote

#+begin_quote
Each time the proposed model generates a word in a translation, it
(soft-)searches for a set of positions in a source sentence where the most
relevant information is concentrated. The model then predicts a target word
*based on the context vectors associated with these source positions*
*and all the previous generated target words.*
#+end_quote

#+begin_quote
it [the model] encodes the input sentence into a sequence of vectors and chooses
a subset of these vectors adaptively while decoding the translation.'
#+end_quote

** learning to align and translate

#+begin_quote
The new architecture consists of a bidirectional RNN as an encoder (Sec. 3.2)
and a decoder that emulates searching through a source sentence during decoding
a translation (Sec.  3.1).
#+end_quote

#+attr_org: :width 700
[[./tex/illustrations/decodergeneraldescritptionAlignment.png]]
Personnal summary:

Each (hidden) state of the decoder RNN depends on a context vector. This context vector
is a weighted sum of the hidden states of the encoder. The weights depend on an alignment
value which scores how relevant each hidden state j is to the current output i. The weights
are normalized to form a probability distribution through a sotfmax function. All in all the
weights are the output of a simple feedforward network with the hidden states of the encoder
as input.

** ref
- [[https://arxiv.org/abs/1409.0473][arxiv Neural Machine Translation by Jointly Learning to Align and Translate]]
- file:./papers/jointlyLearningToAlignAndTranslate.pdf
#+begin_src tex
@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate},
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
#+end_src


* Low ressource transformers
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-05 Fri 14:33
:ARCHIVE_FILE: ~/udem/nlp/project/readme.org
:ARCHIVE_CATEGORY: readme
:END:
** Efficient transformers: a survey
*** introduction
#+begin_quote
There has been such a surge of Transformer model variants proposed recently, that
researchers and practitioners alike may find it challenging to keep pace with the rate of
innovation. As of this writing (circa August 2020), there have been nearly a dozen new
efficiency-focused models proposed in just the past 6 months.
#+end_quote

#+begin_quote
The self-attention mechanism is a key defining characteristic of Transformer models.
The mechanism can be viewed as a graph-like inductive bias that connects all tokens in
a sequence with a relevance-based pooling operation. A well-known concern with self-
attention is the *quadratic time and memory complexity*, which can hinder model scalability
in many settings. There has been an overwhelming influx of model variants proposed
recently that address this problem. We hereinafter name this class of models “efficient
Transformers”.
#+end_quote

*** background on transformers
#+attr_org: :width 600
[[./tex/illustrations/transformerMathSummary.png]]

#+begin_quote
It is important to note the differences in the mode of usage of the Transformer
block.  Transformers can primarily be used in three ways, namely: (1)
encoder-only (e.g., for classification), (2) decoder-only (e.g., for language
modeling), and (3) encoder-decoder (e.g., for machine translation). In
encoder-decoder mode, there are usually multiple multi-headed self-attention
modules, including a standard self-attention in both the encoder and the
decoder, *along with an encoder-decoder cross-attention that allows the decoder
to utilize information from the encoder.* This influences the design of the
self-attention mechanism.

In the encoder mode, there is no restriction or constraint that the
self-attention mechanism has to be causal, i.e., dependent solely on the present
and past tokens.

In the encoder-decoder setting, the encoder and encoder-decoder cross attention
can afford to be non-causal but the *decoder self-attention must be causal.*
#+end_quote

*** a survey of efficient transformers
#+attr_org: :width 800
[[./tex/illustrations/taxonomyEfficientTransformers.png]]

#+begin_quote
The primary goal of most of these models, with the exception of those based on
segment-based recurrence, is to approximate the quadratic- cost attention
matrix. Each method applies some notion of sparsity to the otherwise dense
attention mechanism.
#+end_quote

*** ref
- [[https://arxiv.org/abs/2009.06732][{2009.06732} Efficient Transformers: A Survey]]
- file:./papers/2009.06732.pdf
#+begin_src tex
@misc{tay2020efficient,
      title={Efficient Transformers: A Survey},
      author={Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
      year={2020},
      eprint={2009.06732},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
#+end_src

** ALBERT
*** ref
[[https://openreview.net/forum?id=H1eA7AEtvS][ALBERT: A Lite BERT for Self-supervised Learning of Language Representations ...]]
file:./papers/albert.pdf
#+begin_src tex
@inproceedings{Lan2020ALBERT,
    title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=H1eA7AEtvS}
}

#+end_src
** Reformer
*** ref
[[https://openreview.net/forum?id=rkgNKkHtvB][Reformer: The Efficient Transformer | OpenReview]]
file:./papers/reformer.pdf
#+begin_src tex
@inproceedings{
    Kitaev2020Reformer:,
    title={Reformer: The Efficient Transformer},
    author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rkgNKkHtvB}
}
#+end_src
** Simple Recurrent units
*** ref
[[https://arxiv.org/abs/1709.02755][{1709.02755} Simple Recurrent Units for Highly Parallelizable Recurrence]]
file:./papers/1709.02755.pdf
#+begin_src tex
@article{sru,
  author    = {Tao Lei and
               Yu Zhang and
               Yoav Artzi},
  title     = {Training RNNs as Fast as CNNs},
  journal   = {CoRR},
  volume    = {abs/1709.02755},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.02755},
  archivePrefix = {arXiv},
  eprint    = {1709.02755},
  timestamp = {Mon, 13 Aug 2018 16:46:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-02755.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src

** Sparse Transformers
*** ref
[[https://openai.com/blog/sparse-transformer/][Generative Modeling with Sparse Transformers]]
[[https://arxiv.org/abs/1904.10509][{1904.10509} Generating Long Sequences with Sparse Transformers]]
file:./papers/1904.10509.pdf
#+begin_src tex
@article{sparsetransformers,
  author    = {Rewon Child and
               Scott Gray and
               Alec Radford and
               Ilya Sutskever},
  title     = {Generating Long Sequences with Sparse Transformers},
  journal   = {CoRR},
  volume    = {abs/1904.10509},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.10509},
  archivePrefix = {arXiv},
  eprint    = {1904.10509},
  timestamp = {Thu, 02 May 2019 15:13:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-10509.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src

* Attention concept in general
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-05 Fri 14:33
:ARCHIVE_FILE: ~/udem/nlp/project/readme.org
:ARCHIVE_CATEGORY: readme
:END:
** Attention in Natural Language Processing
*** intro
Four dimensions to the taxonomy:
- the representation of the input,
- the compatibility function,
- the distribution function,
- the multiplicity of the input and/or output.

*** attention models
#+attr_org: :width 700
[[./tex/illustrations/coreAttentionModel.png]]

#+attr_org: :width 800
[[./tex/illustrations/generaelAttentionModel.png]]

*** use of attention
#+begin_quote
Attention enables us to estimate the relevance of the input
elements as well as to combine said elements into a com-
pact representation—the context vector—that condenses the
characteristics of the most relevant elements. Because the
context vector is smaller than the original input, it requires
fewer computational resources to be processed at later stages,
yielding a computational gain.
#+end_quote

#+begin_quote
When the generation of a text sequence is required, as in
machine translation, attention enables us to make use of a
dynamic representation of the input sequence, whereby the
whole input does not have to be encoded into a single vector.
At each time step, the encoding is tailored according to the
task, and in particular, q represents an embedding of the
previous state of the decoder. More generally, the possibility to
perform attention with respect to a query q allows us to create
representations of the input that depend on the task context,
creating specialized embeddings. This is particularly useful in
tasks, such as sentiment analysis and information extraction.
#+end_quote

#+begin_quote
Since attention can create contextual representations of an
element, it can also be used to build sequence-to-sequence
annotators, without resorting to RNNs or convolutional neural
networks (CNNs), as suggested by Vaswani et al. [36],
who rely on an attention mechanism to obtain a whole
encoder/decoder architecture.
#+end_quote

#+begin_quote
Attention can also be used as a tool for selecting specific
words. This could be the case, for example, in dependence
parsing [97] and in cloze question-answering tasks [66], [70].
In the former case, attention can be applied to a sentence in
order to predict dependences. In the latter, attention can be
applied to a textual document or to a vocabulary to perform a
classification among the words.
#+end_quote

#+begin_quote
Finally, attention can come in handy when multiple inter-
acting input sequences have to be considered in combination.
In tasks such as question answering, where the input consists
of two textual sequences—for instance, the question and
the document or the question and the possible answers—an
input encoding can be obtained by considering the mutual
interactions between the elements of such sequences, rather
than by applying a more rigid a priori defined model.
#+end_quote

*** taxonomy for attention models
#+begin_quote
In NLP-related tasks, generally, K and V are representations
of parts of documents, such as sequences of characters, words,
or sentences. These components are usually embedded into
continuous vector representations and then processed through
key/value annotation functions (called kaf and vaf in Fig. 4),
so as to obtain a hidden representation resulting in K and V .
Typical annotation functions are RNN layers such as gated
recurrent units (GRUs), long short-term memory networks
(LSTMs), and CNNs. In this way, k i and v i represent an input
element relative to its local context.
#+end_quote

#+begin_quote
We made a distinction between two input sources: the input sequence, represented
by K and V , and the query, represented by q. However, some architectures
compute attention only based on the input sequence. These architectures are
known as self-attentive or intraattentive mod- els.
#+end_quote


#+begin_quote
The commonest one amounts to the application of multiple steps of attention to a
vector K , using the elements k t of the same vector as query at each step [18],
[36]. At each step, the weights a i t represent the relevance of k i with
respect to k t , yielding d K separate context embeddings, c t , one per key.
#+end_quote

#+begin_quote
Attention could thus be used as a sequence-to-sequence model, as an alternative
to CNNs or RNNs (see Fig. 5). In this way, each element of the new sequence may
be influenced by elements of the whole input, incorporating contextual
information without any locality boundaries. This is especially interesting
since it could overcome a well-known shortcoming of RNNs: their limited ability
of modeling long-range dependences [140]. For each element k t , the resulting
distribution of the weights a t should give more emphasis to words that strongly
relate to k t . The analysis of these distributions will, therefore, provide
information regarding the relationship between the elements inside the sequence.
Modern text-sequence generation systems often rely on this approach
#+end_quote

*** hiarchical attention
#+begin_quote
Hierarchical-Input Architectures: In some tasks, portions
of input data can be meaningfully grouped together into higher
level structures, where hierarchical-input attention models can
be exploited to subsequently apply multiple attention modules
at different levels of the composition, as shown in Fig. 6.
Consider, for instance, data naturally associated with a
two-level semantic structure, such as characters (the “micro”
elements) forming words (the “macro” elements) or words
forming sentences. Attention can be first applied to the rep-
resentations of micro elements k i , so as to build aggregate
representations k j of the macro elements, such as context
vectors. Attention could then be applied again to the sequence
of macroelement embeddings, in order to compute an embed-
ding for the whole document D. With this model, attention
first highlights the most relevant micro elements within each
macro element and, then, the most relevant macro elements in
the document. For instance, Yang et al. [52] applied attention
first at word level, for each sentence in turn, to compute
sentence embeddings. Then, they applied attention again on
the sentence embeddings to obtain a document representation.
7
With reference to the model introduced in Section II, embed-
dings are computed for each sentence in D, and then, all
such embeddings are used together as keys K to compute the
document-level weights a and eventually D’s context vector c.
The hierarchy can be extended further. For instance, Wu et al.
[141] added another layer on top, applying attention also at
the document level.
If representations for both micro- and macro-level elements
are available, one can compute attention on one level and
then exploit the result as a key or query to compute atten-
tion on the other, yielding two different microrepresenta-
tion/macrorepresentation of D. In this way, attention enables
us to identify the most relevant elements for the task at both
levels. The attention-via-attention model by Zhao and Zhang
[43] defines a hierarchy with characters at the micro level and
words at the macro level. Both characters and words act as
keys. Attention is first computed on word embeddings K W ,
thus obtaining a document representation in the form of a
context vector c W , which in turn acts as a query q to guide the
application of character-level attention to the keys (character
embeddings) K C , yielding a context vector c for D.
Ma et al. [113] identified a single “target” macro-object T
as a set of words, which do not necessarily have to form a
sequence in D, and then used such a macro-object as keys,
K T . The context vector c T produced by a first application of
the attention mechanism on K T is then used as query q in
a second application of the attention mechanism over D, with
the keys being the document’s word embeddings K W .
#+end_quote

*** ref
- [[https://arxiv.org/abs/1902.02181][{1902.02181} Attention in Natural Language Processing]]
- file:~/udem/nlp/project/papers/1902.02181.pdf
#+begin_src tex
@article{DBLP:journals/corr/abs-1902-02181,
  author    = {Andrea Galassi and
               Marco Lippi and
               Paolo Torroni},
  title     = {Attention, please! {A} Critical Review of Neural Attention Models
               in Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1902.02181},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.02181},
  archivePrefix = {arXiv},
  eprint    = {1902.02181},
  timestamp = {Wed, 25 Sep 2019 17:52:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-02181.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src

** attention? attention!
*Note: difficult for now might point to fancy improvements to consider in the future*
*** different mechanisms
#+name: A family of attention mechanisms
#+ATTR_ORG: :width 700
[[./tex/illustrations/afamilyofattentionmechanisms.png]]

*** self attention
#+begin_quote
Self-attention, also known as intra-attention, is an attention mechanism
relating different positions of a single sequence in order to compute a
representation of the same sequence. It has been shown to be very useful in
machine reading, abstractive summarization, or image description generation.
#+end_quote

#+begin_quote
In the show, attend and tell paper, attention mechanism is applied to images to
generate captions. The image is first encoded by a CNN to extract features. Then
a LSTM decoder consumes the convolution features to produce descriptive words
one by one, where the weights are learned through attention. The visualization
of the attention weights clearly demonstrates which regions of the image the
model is paying attention to so as to output a certain word.
#+end_quote
show and tell paper: [[https://arxiv.org/abs/1502.03044][{1502.03044} Show, Attend and Tell: Neural Image Caption Generation with Visu...]]

- Soft Attention: the alignment weights are learned and placed “softly” over all
  patches in the source image; essentially the same type of attention as in
  Bahdanau et al., 2015.

  + Pro: the model is smooth and differentiable.
  + Con: expensive when the source input is large.

- Hard Attention: only selects one patch of the image to attend to at a time.

  + Pro: less calculation at the inference time.
  + Con: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (Luong, et al., 2015)

*** global vs local attention
#+name: global vs local attention
#+attr_org: :width 700
[[./tex/illustrations/globalvslocalattention.png]]

reference: [[https://arxiv.org/abs/1508.04025][{1508.04025} Effective Approaches to Attention-based Neural Machine Translation]]
*** multi-head self attention: key value query
*important*
#+begin_quote
The transformer views the encoded representation of the input as a
set of key-value pairs, (K,V), both of dimension n (input sequence length);

in the context of NMT, both the keys and values are the encoder hidden states.
In the decoder, the previous output is compressed into a query (Q of
dimension m) and the next output is produced by mapping this query and the set
of keys and values.
#+end_quote

#+attr_org: :width 700
[[./tex/illustrations/transformerscaleddotproduct.png]]

#+attr_org: :width 700
[[./tex/illustrations/multiheadselfattention.png]]
reference: transformer paper

*important*
#+begin_quote
Rather than only computing the attention once, the multi-head mechanism runs
through the scaled dot-product attention multiple times in parallel.

The independent attention outputs are simply concatenated and linearly transformed
into the expected dimensions.

According to the paper, “multi-head attention allows the model
to jointly attend to information from different representation subspaces at
different positions. With a single attention head, averaging inhibits this.”
#+end_quote

*** TODO snail
*** ref
- [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
#+begin_src tex
@article{weng2018attention,
  title   = "Attention? Attention!",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2018",
  url     = "http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html"
}
#+end_src


* Code relevant links
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-05 Fri 14:33
:ARCHIVE_FILE: ~/udem/nlp/project/readme.org
:ARCHIVE_CATEGORY: readme
:END:
- Transformer
  + [[http://nlp.seas.harvard.edu/2018/04/03/attention.html][The Annotated Transformer]]
  + [[https://becominghuman.ai/attention-is-all-you-need-16bf481d8b5c][Attention is all you need. An explanation about transformer | by Pierrick RUG...]]
  + [[http://vandergoten.ai/2018-09-18-attention-is-all-you-need/][Attention Is All You Need]]

- pretraining bert
  + [[https://d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html][14.9. The Dataset for Pretraining BERT — Dive into Deep Learning 0.16.1 docum...]]
  + [[https://github.com/google-research/bert/issues/750][google-research/bert#750 Creating training set from Wikipedia data?]]
  + [[https://towardsdatascience.com/preparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a][Preparing the data for Transformer pre-training — a write-up | by Steven van ...]]
  + [[https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67][Pre-processing a Wikipedia dump for NLP model training — a write-up | by Stev...]]


* other ressources
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-05 Fri 14:33
:ARCHIVE_FILE: ~/udem/nlp/project/readme.org
:ARCHIVE_CATEGORY: readme
:END:
** Rethinking the value of network pruning
*** ref
[[https://openreview.net/forum?id=rJlnB3C5Ym][Rethinking the Value of Network Pruning | OpenReview]]
file:./papers/rethinking_the_value_of_network_pruning.pdf
#+begin_src tex
@inproceedings{
    liu2018rethinking,
    title={Rethinking the Value of Network Pruning},
    author={Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=rJlnB3C5Ym},
}
#+end_src
** do we really need model compression
*** ref
[[http://mitchgordon.me/machine/learning/2020/01/13/do-we-really-need-model-compression.html][Do We Really Need Model Compression? | Mitchell A. Gordon]]
#+begin_src tex
@misc{gordon_2019,
    author = "Mitchell A. Gordon",
    title = "Do We Really Need Model Compression?",
    year = "2020",
    howpublished="http://mitchgordon.me/machine/learning/2020/01/13/do-we-really-need-model-compression.html",
}
#+end_src
** github links
- [[https://github.com/google-research/bert][GitHub - google-research/bert: TensorFlow code and pre-trained models for BERT]]
- [[https://github.com/huggingface/transformers][GitHub - huggingface/transformers: Transformers: State-of-the-art NLP]]
** non academic links
- easier explanations of transformer
  + [[https://jalammar.github.io/illustrated-transformer/][The Illustrated Transformer – Jay Alammar – Visualizing machine learning one ...]]
- attention
  + [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
  + [[https://theaisummer.com/attention/][How Attention works in Deep Learning: understanding the attention mechanism i...]]
- positional encoding
  + [[https://kazemnejad.com/blog/transformer_architecture_positional_encoding/][Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's ...]]
  + [[https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model][nlp - What is the positional encoding in the transformer model? - Data Scienc...]]
- machine translation through alignment
  + [[https://www.tensorflow.org/tutorials/text/nmt_with_attention][Neural machine translation with attention  |  TensorFlow Core]]
- videos
  + [[https://www.youtube.com/watch?v=rBCqOTEfxvg][Attention is all you need; Attentional Neural Network Models | Łukasz Kaiser ...]]
  + [[https://www.youtube.com/watch?v=-QH8fRhqFHM][The Narrated Transformer Language Model - YouTube]]
  + [[https://www.youtube.com/watch?v=S27pHKBEp30][LSTM is dead. Long Live Transformers! - YouTube]]

** misc

- [[https://github.com/hlissner/doom-emacs][GitHub - hlissner/doom-emacs: An Emacs framework for the stubborn martian hacker]]
- [[https://orgmode.org/worg/org-tutorials/][Org tutorials]]
- [[https://org-babel.readthedocs.io/en/latest/header-args/][Header arguments - Org Babel reference card]]

- [[https://www.overleaf.com/3324588169zyyzyysrrtmw][Overleaf, Online LaTeX Editor]]
- [[https://www.overleaf.com/learn/latex/bibliography_management_with_bibtex][Bibliography management with bibtex - Overleaf, Online LaTeX Editor]]
- [[https://www.overleaf.com/learn/latex/Inserting_Images][Inserting Images - Overleaf, Online LaTeX Editor]]

* link dump
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-05 Fri 14:37
:ARCHIVE_FILE: ~/udem/nlp/project/readme.org
:ARCHIVE_OLPATH: model compression
:ARCHIVE_CATEGORY: readme
:END:
- [[https://www.pragmatic.ml/a-survey-of-methods-for-model-compression-in-nlp/][A Survey of Methods for Model Compression in NLP]]
- [[https://arxiv.org/abs/2002.12620][{2002.12620} TextBrewer: An Open-Source Knowledge Distillation Toolkit for Na...]]

* TODO towards midway report
:PROPERTIES:
:ARCHIVE_TIME: 2021-02-05 Fri 14:43
:ARCHIVE_FILE: ~/udem/nlp/project/readme.org
:ARCHIVE_OLPATH: project proposal
:ARCHIVE_CATEGORY: readme
:ARCHIVE_TODO: TODO
:END:
** TODO understand how to use hugging face transformers library
** TODO understand code of transformer
** TODO get datasets and preprocessing
** TODO understand (fine-tuning) training for question answering
