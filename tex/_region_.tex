\message{ !name(projectproposal.tex)}\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{comment}
\usepackage{booktabs}


\usepackage[table]{xcolor}
\usepackage{longtable}
\usepackage{geometry}
\geometry{margin=1in}
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{colorlinks, linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

% Logo on header
\usepackage{graphicx}
\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{1pt}
\fancypagestyle{plain}{%
  \fancyhead[L]{
    \begin{tabular}{ll}
      \begin{tabular}[t]{c}
        \includegraphics[scale=0.06]{UdeM.png}%
      \end{tabular} &
      \begin{tabular}[b]{l}
        Faculté des arts et des sciences\tabularnewline
        Département d'\textbf{informatique et de recherche opérationnelle}  \tabularnewline
      \end{tabular}
    \end{tabular}
  }%
}

% Title
\title{\textbf{Pretrained Language Models on Low Ressources}}
\author{Gao Yinghan, Wei Wei, Frederic Boileau}

\begin{document}

\message{ !name(projectproposal.tex) !offset(-3) }

\thispagestyle{plain}
\maketitle
\medskip




\section{The Transformer revolution}
Vaswani et al \cite{allyouneed} introduced a revolutionnary
architecture in 2017, the transformer. Still based on the idea of an encoder and
decoder for transduction, they based their architecture solely on an attention
mechanism enables to model the long range dependencies in a sentence without the
inherently sequential constraint which RNNs in their various forms imply. The
massive parallelization enabled massive improvements in training time to achieve
state of the art results in machine translation\\


\subsection{Pretrained Language Models}
We will focus in our project not precisely on transduction tasks but on large
pretrained language models (PLMs) which can then be fined tuned for downstream
tasks such as \ldots\\


\subsection{BERT}
BERT is \ldots\cite{bert}\\


But those models tend to be huge \ldots
\begin{table}[h]
  \centering
  \begin{tabular}{cc}
    \toprule
    Architecture & Number of parameters\\
    \midrule
    BERT & 340M\\
    GPT-2 & 1.5B\\
    MegatronLM & 8.3B\\
    T5 & 11B\\
    T-NLG & 17B\\
    GShard & 600B
    \bottomrule
  \end{tabular}
  \caption{PLMs and their sizes}\cite{gupta2020compression}
\end{table}



\clearpage
\section{Compression approaches}
Model compression consists of \ldots

\subsection{Knowledge distillation}
Knowledge Distillation (KD)\ldots\cite{hinton2015distilling}

\subsection{tinyBERT}
TinyBERT\ldots\cite{tinybert}

\subsection{Other model commpression techniques}
We have the following other model compression techniques\ldots\cite{gupta2020compression}
\paragraph{Pruning}\ldots
\paragraph{Quantization}\ldots
\paragraph{Parameter Sharing}\ldots
\paragraph{Tensor Decomposition}\ldots


\section{Experiments and Evaluation}
PLMs can be evaluated on GLUE for general LM performance and more specific downstream
tasks for which they are fine tuned such as Squad.
  \paragraph{SquAD}\ldots\cite{squad}
  \paragraph{GLUE}\ldots\cite{glue}


\clearpage
\bibliography{projectbibliography}
\bibliographystyle{unsrt}



\begin{comment}


\section*{Introduction to attention and its history}

It was conjectured by Bahdanau et al\cite{bahdanau2016neural} that the dominant
encoder decoder paradigms to Neural Machine Translation presented one major
drawback. All the information necessary to the transduction mechanism had to be
encoded into a fixed length vector. The dominant approach was to use a RNN to
encode the sequence to transduce (or more precisely the sentence to translate in
the case of Bahdanau et al) into a fixed length vector c which was a function of
the encoder's hiddens states, usually simply extracting the last hidden
parameter. To remedy the situation the authors tackled the problem with an
innovative approach: making the context vector depend on a linear combinations
of \textbf{all} the hidden states, where the weights assigned respectively were
learned through training a feedforward network. This enabled to effectively
learn what part of the input sentence the output had to \textit{attend to}. This
idea was called attention.

\begin{equation}
  \label{eq:3}
  c_{i} = \sum_{j=1}^{T_{x}}\alpha_{ij}h_{j} \qquad
  \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_{x}}\exp(e_{ik})} \qquad
  e_{ij} = a(s_{i-1}, h_{j})
\end{equation}
% TODO explain in words fundamental equations of attention





\begin{itemize}
  \item Albert\cite{Lan2020ALBERT}
  \item Reformer\cite{Kitaev2020Reformer:}
  \item Simple Recurrent Units (SRU)\cite{sru}
  \item Sparse Transformers (ST)\cite{sparsetransformers}
\end{itemize}

\end{comment}

\end{document}

\message{ !name(projectproposal.tex) !offset(-168) }
