\message{ !name(projectproposal.tex)}\documentclass{article}

\usepackage{csquotes}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{comment}
\usepackage{booktabs}


\usepackage[table]{xcolor}
\usepackage{longtable}
\usepackage{geometry}
\geometry{margin=1in}
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{colorlinks, linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

% Logo on header
\usepackage{graphicx}
\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{1pt}
\fancypagestyle{plain}{%
  \fancyhead[L]{
    \begin{tabular}{ll}
      \begin{tabular}[t]{c}
        \includegraphics[scale=0.06]{UdeM.png}%
      \end{tabular} &
      \begin{tabular}[b]{l}
        Faculté des arts et des sciences\tabularnewline
        Département d'\textbf{informatique et de recherche opérationnelle}  \tabularnewline
      \end{tabular}
    \end{tabular}
  }%
}

% Title
\title{\textbf{Pretrained Language Models on Low Ressources}}
\author{Gao Yinghan, Wei Wei, Frederic Boileau}

\begin{document}

\message{ !name(projectproposal.tex) !offset(-3) }

\thispagestyle{plain}
\maketitle
\medskip


\paragraph{Transformer} Vaswani et al \cite{allyouneed} introduced a
revolutionnary architecture for machine translation in 2017, the transformer.
The idea was based on the idea of an encoder and decoder for transduction as for
the state of the art predecessors relying on deep bidirectionnal RNNs for both
the encoding and the decoding stages. The authors of the transformer paper,
``Attention is all you need'' based their architecture's sequence modeling
solely on an attention mechanism which enables it to model the long range
dependencies in a sentence without the inherently sequential constraint which
RNNs in their various forms imply. The massive parallelization enabled massive
improvements in training time to achieve state of the art results in machine
translation amongst other NLP tasks.\\

Attention in its more general form is most succintly put by Vaswani et al in their paper:
\blockcquote{allyouneed}{An attention function can be described as mapping a
query and a set of key-value pairs to an output, where the query, keys, values,
and output are all vectors. The output is computed as a weighted sum of the
values, where the weight assign}\\

More formally, the key equation is:
\begin{equation}
  \label{eq:1}
  \mathrm{Attention(Q,K,V) =
    \mathrm{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)}
\end{equation}


% TODO self attention equation

\paragraph{Pretrained Language Models}Bidirectional Encoder Representation from
(BERT)\cite{bert} is an architecture based on the encoder module of the
transformer. Its training is done in two steps, first it learns a Language Model
(LM) which results in a Pretrained Language Model (PLM) through some
unsupervised learning tasks. The training procedure is done over two tasks:
Masked Language Model (MLM) and Next Sentence Prediction (NSP). The MLM task
simply trains the model to learn to predict randomly masked words from a
sentence. In NSP, BERT learns to predict, given a pair of sentences, whether one
follows the other. When trained on those tasks the BERT yields a PLM. The
resulting model can then be fine-tuned in a supervised setting for a specific
tasks such as question answering. This ability to train once over a huge corpus
of data to yield a LM and then fine tune it downstream adresses one core issue
of deep learning: how to transfer learning or knowledge?
BERT is just one of many examples of large PLM deployed today. The
adjoined table lists the models and their associated number of parameters.
\begin{table}[htbp]
\centering
  \begin{tabular}{cc}
    \toprule
    Architecture & Number of parameters\\
    \midrule
    BERT & 340M\\
    GPT-2 & 1.5B\\
    MegatronLM & 8.3B\\
    T5 & 11B\\
    T-NLG & 17B\\
    GShard & 600B\\
    \bottomrule
  \end{tabular}
  \caption{PLMs and their sizes\cite{gupta2020compression}}
\end{table}

\clearpage
\paragraph{Knowledge Distillation} Knowledge Distillation (KD) addresse the
following issue, how can we leverage the state of the art (SOA) results given by
large PLMs to do inference in a context where memory and computing power are
limited. One avenue would be to use one of the large PLMs to ``teach'' a smaller
model (the student) which we can deploy in more ressource limited environments.
Hinton, though not specifically with respect argued in
2015\cite{hinton2015distilling} that one conceptual roadblock to knowledge
transfer or ``distillation'' had been the rigid identification of a model with
the learned parameter values instead of the more abstract view of a
\enquote{learned mapping from input vectors to output
vectors}\cite{hinton2015distilling}. The way to do this according to Hinton et
al is to make the student learn through an objective function which reflects the
generalization ability learned in the teacher model. To achieve this he proposes
using an objective function which averages over the soft target (the output
probabilities of the teacher where the logits are divided by temperature factor
to adjust the smoothness of those targets) and the ground truth.\\

\paragraph{TinyBERT}\ldots\cite{tinybert}

\paragraph{Further model compression}
We have the following other model compression techniques\ldots\cite{gupta2020compression}
\begin{itemize}
  \item Pruning\ldots
  \item Quantization\ldots
  \item Parameter Sharing\ldots
  \item Tensor Decomposition\ldots
\end{itemize}


\paragraph{Evaluation}
\begin{itemize}
  \item Squad\cite{squad}
  \item GLUE\cite{glue}
\end{itemize}


\bibliography{projectbibliography}
\bibliographystyle{unsrt}



\begin{comment}


\section*{Introduction to attention and its history}

It was conjectured by Bahdanau et al\cite{bahdanau2016neural} that the dominant
encoder decoder paradigms to Neural Machine Translation presented one major
drawback. All the information necessary to the transduction mechanism had to be
encoded into a fixed length vector. The dominant approach was to use a RNN to
encode the sequence to transduce (or more precisely the sentence to translate in
the case of Bahdanau et al) into a fixed length vector c which was a function of
the encoder's hiddens states, usually simply extracting the last hidden
parameter. To remedy the situation the authors tackled the problem with an
innovative approach: making the context vector depend on a linear combinations
of \textbf{all} the hidden states, where the weights assigned respectively were
learned through training a feedforward network. This enabled to effectively
learn what part of the input sentence the output had to \textit{attend to}. This
idea was called attention.

\begin{equation}
  \label{eq:3}
  c_{i} = \sum_{j=1}^{T_{x}}\alpha_{ij}h_{j} \qquad
  \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_{x}}\exp(e_{ik})} \qquad
  e_{ij} = a(s_{i-1}, h_{j})
\end{equation}
% TODO explain in words fundamental equations of attention





\begin{itemize}
  \item Albert\cite{Lan2020ALBERT}
  \item Reformer\cite{Kitaev2020Reformer:}
  \item Simple Recurrent Units (SRU)\cite{sru}
  \item Sparse Transformers (ST)\cite{sparsetransformers}
\end{itemize}

\end{comment}

\end{document}

\message{ !name(projectproposal.tex) !offset(-199) }
