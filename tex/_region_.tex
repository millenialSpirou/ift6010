\message{ !name(projectproposal.tex)}\documentclass{article}

\usepackage{csquotes}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{comment}
\usepackage{booktabs}


\usepackage[table]{xcolor}
\usepackage{longtable}
\usepackage{geometry}
\geometry{margin=1in}
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{colorlinks, linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

% Logo on header
\usepackage{graphicx}
\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{1pt}
\fancypagestyle{plain}{%
  \fancyhead[L]{
    \begin{tabular}{ll}
      \begin{tabular}[t]{c}
        \includegraphics[scale=0.06]{UdeM.png}%
      \end{tabular} &
      \begin{tabular}[b]{l}
        Faculté des arts et des sciences\tabularnewline
        Département d'\textbf{informatique et de recherche opérationnelle}  \tabularnewline
      \end{tabular}
    \end{tabular}
  }%
}

% Title
\title{\textbf{Applying further model compression to TinyBERT}}
\author{Gao Yinghan, Wei Wei, Frederic Boileau}

\begin{document}

\message{ !name(projectproposal.tex) !offset(-3) }

\thispagestyle{plain}
\maketitle
\medskip


\paragraph{Transformer} Vaswani et al \cite{allyouneed} introduced a
revolutionnary architecture for machine translation in 2017, the
\textit{transformer}.  The idea was based on the idea of an encoder and decoder
in the same vein as the state of the art predecessors which themselves relied on
deep bidirectionnal RNNs for both the encoding and the decoding stages. The idea
of using attention mechanisms was already pioneered for neural machine
translation (NMT) by Bachdanau et al. (2016)\cite{bahdanau2016neural} though
they still relied on RNN's for encoders and decoders. The authors of the
transformer paper, ``Attention is all you need'', based their architecture's
sequence modeling solely on an (self) attention mechanism which enables it to
model the long range dependencies in a sentence without the inherently
sequential constraint which RNNs in their various forms imply. This enabled
massive parallelization which in turn led to huge improvements in training time
(pace model size) to achieve state of the art results in machine translation
amongst other NLP tasks.  Attention in its more general form is most succintly
put by Vaswani et al in their paper: \blockcquote{allyouneed}{An attention
function can be described as mapping a query and a set of key-value pairs to an
output, where the query, keys, values, and output are all vectors.  The output
is computed as a weighted sum of the values, where the weight assigned to each
value is computed by a compatibility function of the
query with the corresponding key.}

\begin{comment}
More formally, the key equation is:
\begin{equation}
  \label{eq:1}
  \mathrm{Attention(Q,K,V) =
    \mathrm{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)}
\end{equation}
where $d_{k}$ is the dimensionnality of the queries and keys. The motivation
for this scaling is \blockcquote{allyouneed}{We suspect that for large values of
$d_{k}$, the dot products grow large in magnitude, pushing the softmax function
into regions where it has extremely small gradients}
\end{comment}



\paragraph{Pretrained Language Models}Bidirectional Encoder Representation from
(BERT)\cite{bert} is an architecture based on the encoder module of the
transformer. Its training is done in two steps, first it learns a Language Model
(LM) which results in a Pretrained Language Model (PLM) through some
unsupervised learning tasks. The training procedure is done over two tasks:
Masked Language Model (MLM) and Next Sentence Prediction (NSP). The MLM task
simply trains the model to learn to predict randomly masked words from a
sentence. In NSP, BERT learns to predict, given a pair of sentences, whether one
follows the other. When trained on those tasks the BERT yields a PLM. The
resulting model can then be fine-tuned in a supervised setting for a specific
tasks such as question answering. This ability to train once over a huge corpus
of data to yield a LM and then fine tune it downstream adresses one core issue
of deep learning: how to transfer learning or knowledge?
BERT is just one of many examples of large PLM deployed today. The
adjoined table lists the models and their associated number of parameters.

\clearpage
\paragraph{Knowledge Distillation} Knowledge Distillation (KD) addresse the
following issue, how can we leverage the state of the art (SOA) results given by
large PLMs to do inference in a context where memory and computing power are
limited. One avenue would be to use one of the large PLMs to ``teach'' a smaller
model (the student) which we can deploy in more ressource limited environments.
Hinton, though not specifically with respect to PLMs, argued in
2015\cite{hinton2015distilling} that one conceptual roadblock to knowledge
transfer or ``distillation'' had been the rigid identification of a model with
the learned parameter values instead of the more abstract view of a
\enquote{learned mapping from input vectors to output
vectors}\cite{hinton2015distilling}. The way to do this according to Hinton et
al is to make the student learn through an objective function which reflects the
generalization ability learned in the teacher model. To achieve this he proposes
using an objective function which averages over the soft target (the output
probabilities of the teacher where the logits are divided by temperature factor
to adjust the smoothness of those targets) and the ground truth.\\


\paragraph{Application of KD} The idea of KD is a fertile one and has been
applied to BERT to train a model called TinyBERT\cite{tinybert}. In this paper
the authors \blockcquote{tinybert}{introduce a new two-stage learning framework
for TinyBERT, which per- forms Transformer distillation at both the pre-training
and task-specific learning stages.} This enables the model to learn both general
LM features and more downstream tasks. They also propose three types of loss
functions which learn from different parameters of the teacher, namely the
output of the embedding layer, the hidden states and attention matrices and the
logits output by the prediction layer. In choosing to learn direclty from the
attention matrices the authors are inspired by the work of Clark et al.
(2019)\cite{whatdoesbertlookat} which shows that the former can
\blockcquote{tinybert}{substantial linguistic knowledge}. With those
aforementionned methods tinyBERT \blockcquote{tinybert}{while being
7.5x smaller and 9.4x faster on inference.}


\paragraph{Further model compression} In the conclusion of their paper on
tinyBERT the authors mention that combining their distillation strategy with
further model compression could be an interesting direction. They especially
mention \textbf{weight pruning} and \textbf{quantization} as promising avenues.
However the literature on model compression is rich and the excellent survey by
Gupta and Agarwal (2020) \cite{gupta2020compression} mentions parameter sharing
and tensor decomposition as well as interesting model compression paradigms.
Their taxonomy is rich and for a bird's eye view we refer the reader to the
figure\ref{fig:modelcompression} they supply with their paper. Even within
each of these four paradigms
\begin{itemize}
  \item Pruning
  \item Quantization
  \item Parameter Sharing
  \item Tensor Decomposition
\end{itemize}
Many sub-paradigms of those four are described in Gupta and Agarwal (2020)

\paragraph{Proposal} In light of the previous discussion we propose to
experiment with different paradigms of model compression on top of the KD based
one implemented by TinyBERT. We still have yet to pick the most promising avenue
for further investigations, which is the next step in our project.


\paragraph{Evaluation} We will evaluate our models on two general criteria,
performance on standardized tasks and memory and computational ressources required
for training and inference at deployment. The two framworks for evaluating performance
we are considering are listed below with reference.
\begin{itemize}
  \item Squad\cite{squad}
  \item GLUE\cite{glue}
\end{itemize}


\clearpage
\bibliography{projectbibliography}
\bibliographystyle{unsrt}

\clearpage
\appendix
\section{Appendix}

\begin{table}[htbp]
\centering
  \begin{tabular}{cc}
    \toprule
    Architecture & Number of parameters\\
    \midrule
    BERT & 340M\\
    GPT-2 & 1.5B\\
    MegatronLM & 8.3B\\
    T5 & 11B\\
    T-NLG & 17B\\
    GShard & 600B\\
    \bottomrule
  \end{tabular}
  \caption{PLMs and their sizes\cite{gupta2020compression}}\label{plmsize}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{illustrations/modelcompressiontaxonomy}
  \caption{Model Compression Taxonomy\cite{gupta2020compression}}\label{fig:modelcompression}
\end{figure}

\begin{comment}
\section*{Introduction to attention and its history}
It was conjectured by Bahdanau et al\cite{bahdanau2016neural} that the dominant
encoder decoder paradigms to Neural Machine Translation presented one major
drawback. All the information necessary to the transduction mechanism had to be
encoded into a fixed length vector. The dominant approach was to use a RNN to
encode the sequence to transduce (or more precisely the sentence to translate in
the case of Bahdanau et al) into a fixed length vector c which was a function of
the encoder's hiddens states, usually simply extracting the last hidden
parameter. To remedy the situation the authors tackled the problem with an
innovative approach: making the context vector depend on a linear combinations
of \textbf{all} the hidden states, where the weights assigned respectively were
learned through training a feedforward network. This enabled to effectively
learn what part of the input sentence the output had to \textit{attend to}. This
idea was called attention.

\begin{equation}
  \label{eq:3}
  c_{i} = \sum_{j=1}^{T_{x}}\alpha_{ij}h_{j} \qquad
  \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_{x}}\exp(e_{ik})} \qquad
  e_{ij} = a(s_{i-1}, h_{j})
\end{equation}
% TODO explain in words fundamental equations of attention
\begin{itemize}
  \item Albert\cite{Lan2020ALBERT}
  \item Reformer\cite{Kitaev2020Reformer:}
  \item Simple Recurrent Units (SRU)\cite{sru}
  \item Sparse Transformers (ST)\cite{sparsetransformers}
\end{itemize}
\end{comment}

\begin{comment}
Formally, borrowing the notation from Hinton et al let $z_{i}$ and $v_{i}$
respectively represent the logits of the teacher and student model and assume
the transfer training is done at temperature $T$. Then the gradient
the cross-entropy gradient of the student with
respect to its logit $z_{i}$,  $DC_{z_{i}}$ can be expressed as
\begin{equation}
  \label{eq:2}

\end{equation}
\end{comment}
\end{document}

\message{ !name(projectproposal.tex) !offset(-252) }
