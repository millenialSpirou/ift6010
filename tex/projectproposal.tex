\documentclass{article}

\usepackage{csquotes}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{comment}
\usepackage{booktabs}



\usepackage[table]{xcolor}
\usepackage{longtable}
\usepackage{geometry}
\geometry{margin=1in}
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{colorlinks, linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

% Logo on header
\usepackage{graphicx}
\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{1pt}
\fancypagestyle{plain}{%
  \fancyhead[L]{
    \begin{tabular}{ll}
      \begin{tabular}[t]{c}
        \includegraphics[scale=0.06]{UdeM.png}%
      \end{tabular} &
      \begin{tabular}[b]{l}
        Faculté des arts et des sciences\tabularnewline
        Département d'\textbf{informatique et de recherche opérationnelle}  \tabularnewline
      \end{tabular}
    \end{tabular}
  }%
}

% Title
\title{\textbf{Applying further model compression to TinyBERT}}
\author{Gao Yinghan, Wei Wei, Frederic Boileau}

\begin{document}
\thispagestyle{plain}
\maketitle
\medskip

\begin{abstract}
    Attention based pretrained-language models (PLM) now dominate
  the state of the art (SOA) in NLP for language modelling tasks such as GLUE and
  more downstream ones like question answering (Squad) or question generation. The
  models however tend to be huge, the pretraining itself it not possible to
  experiment with unless one has access to an array of costly hardware
  accelerators. PLMs on the other hand can be fine tuned after pre-training for
  more specific tasks such as question answering. The problem still stands as the
  models are huge, while BERT has \textit{only} 340M, others are much larger, such
  as Amazon's MegatronLM have 8.3B and for true exageration one can look at
  GShard, a swooping 600B. This poses challenges for deployment as even once
  trained inference and simply storing these models are costly.  We thus tackle
  \textit{model compression}. One very promising model issued from such an idea is
  TinyBERT which used knowledge distillation (KD). We will investigate how such a
  model can be further compressed with techniques such as \textit{pruning} and
  \textit{quantization}
\end{abstract}

\section{Introcuction}


\paragraph{Pretrained Language Models}Bidirectional Encoder Representation from
(BERT)\cite{bert} is an architecture based on the encoder module of the
transformer. Its training is done in two steps, first it learns a Language Model
(LM) which results in a Pretrained Language Model (PLM) through some
unsupervised learning tasks. The training procedure is done over two tasks:
Masked Language Model (MLM) and Next Sentence Prediction (NSP). The MLM task
simply trains the model to learn to predict randomly masked words from a
sentence. In NSP, BERT learns to predict, given a pair of sentences, whether one
follows the other. When trained on those tasks the BERT yields a PLM. The
resulting model can then be fine-tuned in a supervised setting for a specific
tasks such as question answering. This ability to train once over a huge corpus
of data to yield a LM and then fine tune it downstream adresses one core issue
of deep learning: how to transfer learning or knowledge?
BERT is just one of many examples of large PLM deployed today. The
adjoined table lists the models and their associated number of parameters.

\paragraph{Knowledge Distillation} Knowledge Distillation (KD) addresse the
following issue, how can we leverage the state of the art (SOA) results given by
large PLMs to do inference in a context where memory and computing power are
limited. One avenue would be to use one of the large PLMs to ``teach'' a smaller
model (the student) which we can deploy in more ressource limited environments.
Hinton, though not specifically with respect to PLMs, argued in
2015\cite{hinton2015distilling} that one conceptual roadblock to knowledge
transfer or ``distillation'' had been the rigid identification of a model with
the learned parameter values instead of the more abstract view of a
\enquote{learned mapping from input vectors to output
vectors}\cite{hinton2015distilling}. The way to do this according to Hinton et
al is to make the student learn through an objective function which reflects the
generalization ability learned in the teacher model. To achieve this he proposes
using an objective function which averages over the soft target (the output
probabilities of the teacher where the logits are divided by temperature factor
to adjust the smoothness of those targets) and the ground truth.

\section{Related Work}

\paragraph{TinyBERT} The idea of KD is a fertile one and has been
applied to BERT to train a model called TinyBERT\cite{tinybert}. In this paper
the authors \blockcquote{tinybert}{introduce a new two-stage learning framework
for TinyBERT, which performs Transformer distillation at both the pre-training
and task-specific learning stages.} This enables the model to learn both general
LM features and more downstream tasks. They also propose three types of loss
functions which learn from different parameters of the teacher, namely the
output of the embedding layer, the hidden states and attention matrices and the
logits output by the prediction layer. In choosing to learn direclty from the
attention matrices the authors are inspired by the work of Clark et al.
(2019)\cite{whatdoesbertlookat} which shows that the former can
\blockcquote{tinybert}{substantial linguistic knowledge}. With those
aforementionned methods tinyBERT \blockcquote{tinybert}{ achieves more than
96.8\% the performance of its teacher BERT BASE on GLUE benchmark,  while
being 7.5x smaller and 9.4x faster on inference.}


\section{Task and Dataset}

The two framworks for evaluating performance we are considering are
Squad\cite{squad} and GLUE\cite{glue}


\section{Methods}

\paragraph{General approach}In light of the previous discussion we propose to
experiment with different paradigms of model compression on top of the KD based
one implemented by TinyBERT. The strategy outlined below is mainly inspired by
Gupta et al \cite{gupta2020compression}.  We are considering augmenting the
compression of TinyBERT through a mix of quantization and pruning.  Gupta et al
suggest that \blockcquote{gupta2020compression}{Mixed-precision quantization
combined with pruning is highly effective for Transformer based models.}

\paragraph{Pruning} In the case of pruning it is recommended to do the process
iteratively, over epochs during traing, this is called iterative or gradual
pruning. Different patterns of controlling this process exist but they are
independent of the categories of pruning we now describe and we privilege
gradual over static pruning. Unstructured weight pruning (e.g. eliminating low
magnitude weights) leads to difficulties in manipulating the resulting sparse
data structures.  Neuron pruning doesn't yield the same difficulty but is
limited since we need to eliminate whole columns or rows of weight matrices. A
more promising approach is to prune blocks which are contiguously stored in
memory. The blocks to be pruned can be guided through group Lasso
regularization. The latter has been already experimented, however it was
targetting RNN based models and not transformers\cite{blocksparse} We plan on
first experimenting with simple structured block pruning and more intricate
schemes describe in Gupta given we have enough time.

\paragraph{Quantization} With regards to quantization while binary quantization
does not work effectively for text-based Neural networks, ternary and higher-bit
quantization \blockcquote{gupta2020compression}{lead to significant model size
reduction without loss in accuracy across tasks}. Moreover more fancy,
non-uniform, schemes can be used such as the ones based on KMeans or loss aware
schemes.  We plan on starting with uniform quantization and given time available
experiment with non-uniform schemes.

\section{Baselines and evaluation} The resulting compressed model will be first
and foremost compared to TinyBERT and BERT. We will evaluate our models on two
general criteria, performance on standardized tasks and memory and computational
ressources required for training and inference at deployment.




\clearpage
\bibliography{projectbibliography}
\bibliographystyle{unsrt}

\clearpage
\appendix
\section{Appendix}

\begin{table}[htbp]
\centering
  \begin{tabular}{cc}
    \toprule
    Architecture & Number of parameters\\
    \midrule
    BERT & 340M\\
    GPT-2 & 1.5B\\
    MegatronLM & 8.3B\\
    T5 & 11B\\
    T-NLG & 17B\\
    GShard & 600B\\
    \bottomrule
  \end{tabular}
  \caption{PLMs and their sizes\cite{gupta2020compression}}\label{plmsize}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{illustrations/modelcompressiontaxonomy}
  \caption{Model Compression Taxonomy\cite{gupta2020compression}}\label{fig:modelcompression}
\end{figure}

\begin{comment}
More formally, the key equation is:
\begin{equation}
  \label{eq:1}
  \mathrm{Attention(Q,K,V) =
    \mathrm{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)}
\end{equation}
where $d_{k}$ is the dimensionnality of the queries and keys. The motivation
for this scaling is \blockcquote{allyouneed}{We suspect that for large values of
$d_{k}$, the dot products grow large in magnitude, pushing the softmax function
into regions where it has extremely small gradients}
\end{comment}

\begin{comment}
\section*{Introduction to attention and its history}
It was conjectured by Bahdanau et al\cite{bahdanau2016neural} that the dominant
encoder decoder paradigms to Neural Machine Translation presented one major
drawback. All the information necessary to the transduction mechanism had to be
encoded into a fixed length vector. The dominant approach was to use a RNN to
encode the sequence to transduce (or more precisely the sentence to translate in
the case of Bahdanau et al) into a fixed length vector c which was a function of
the encoder's hiddens states, usually simply extracting the last hidden
parameter. To remedy the situation the authors tackled the problem with an
innovative approach: making the context vector depend on a linear combinations
of \textbf{all} the hidden states, where the weights assigned respectively were
learned through training a feedforward network. This enabled to effectively
learn what part of the input sentence the output had to \textit{attend to}. This
idea was called attention.

\begin{equation}
  \label{eq:3}
  c_{i} = \sum_{j=1}^{T_{x}}\alpha_{ij}h_{j} \qquad
  \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_{x}}\exp(e_{ik})} \qquad
  e_{ij} = a(s_{i-1}, h_{j})
\end{equation}
% TODO explain in words fundamental equations of attention
\begin{itemize}
  \item Albert\cite{Lan2020ALBERT}
  \item Reformer\cite{Kitaev2020Reformer:}
  \item Simple Recurrent Units (SRU)\cite{sru}
  \item Sparse Transformers (ST)\cite{sparsetransformers}
\end{itemize}
\end{comment}

\begin{comment}
Formally, borrowing the notation from Hinton et al let $z_{i}$ and $v_{i}$
respectively represent the logits of the teacher and student model and assume
the transfer training is done at temperature $T$. Then the gradient
the cross-entropy gradient of the student with
respect to its logit $z_{i}$,  $DC_{z_{i}}$ can be expressed as
\begin{equation}
  \label{eq:2}

\end{equation}
\end{comment}


\begin{comment}
  \paragraph{Further model compression} In the conclusion of their paper on
tinyBERT the authors mention that combining their distillation strategy with
further model compression could be an interesting direction. They especially
mention \textbf{weight pruning} and \textbf{quantization} as promising avenues.
However the literature on model compression is rich and the excellent survey by
Gupta and Agarwal (2020)\cite{gupta2020compression} mentions parameter sharing
and tensor decomposition as well as interesting model compression paradigms.
The four major paradigms, besides KD, of model compression are the following:
pruning, quantization, parameter sharing and tensor decomposition. Many sub-paradigms
of those four are described in Gupta and Agarwal (2020); their taxonomy is rich
and for a bird's eye view we refer the reader to the
figure\ref{fig:modelcompression} shown in appendix.

\end{comment}
\end{document}
