\documentclass{article}
\usepackage[table]{xcolor}
\usepackage{longtable}
\usepackage{geometry}
\geometry{margin=1in}
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{colorlinks, linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

% Logo on header
\usepackage{graphicx}
\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{1pt}
\fancypagestyle{plain}{%
  \fancyhead[L]{
    \begin{tabular}{ll}
      \begin{tabular}[t]{c}
        \includegraphics[scale=0.06]{UdeM.png}%
      \end{tabular} &
      \begin{tabular}[b]{l}
        Faculté des arts et des sciences\tabularnewline
        Département d'\textbf{informatique et de recherche opérationnelle}  \tabularnewline
      \end{tabular}
    \end{tabular}
  }%
}

% Title
\title{\textbf{Transformer model for question answering, a review}}
\author{Gao Yinghan, Wei Wei, Frederic Boileau}

\begin{document}
\maketitle
\thispagestyle{plain}

``The dominant approach to date encodes the input sequence with a se- ries of
bi-directional recurrent neural networks (RNN) and generates a variable length
output with another set of de- coder RNNs, both of which interface via a
*soft-attention mechanism* (Bahdanau et al., 2014; Luong et al., 2015).''

``Compared to recurrent layers, convolutions create representations for fixed
size contexts, however, the effective context size of the network can easily
be made larger by stacking several layers on top of each other. This allows to
precisely control the maximum length of dependencies to be modeled.''

``Convolutional networks do not depend on the computations of the previous time
step and therefore allow parallelization over every element in a sequence.
This contrasts with RNNs which maintain a hidden state of the entire past that
prevents parallel computation within a sequence.''

``Multi-layer convolutional neural networks create hierarchi- cal representations
over the input sequence in which nearby input elements interact at lower layers
while distant ele- ments interact at higher layers. Hierarchical structure pro-
vides a shorter path to capture long-range dependencies compared to the chain
structure modeled by recurrent networks''\cite{gehring2017convolutional}

\medskip

\bibliography{proposalbibliography}
\bibliographystyle{unsrt}
\end{document}
