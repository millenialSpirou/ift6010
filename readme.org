#+TITLE: IFT6010 project
#+author: frederic boileau
#+STARTUP: inlineimages


* Neural machine translation by jointly learning to align and translate (Bahdanau et all 2014)

** intro

#+begin_quote
In this paper, we conjecture that the use of a fixed-length vector is a
bottleneck in improving the performance of this basic encoder–decoder architec-
ture, and propose to extend this by allowing a model to automatically
(soft-)search for parts of a source sentence that are relevant to predicting a
target word, without having to form these parts as a hard segment explicitly.
#+end_quote

#+begin_quote
Cho et al. (2014b) showed that indeed the performance of a basic
encoder–decoder deteriorates rapidly as the length of an input sentence
increases.
#+end_quote

#+begin_quote
Each time the proposed model generates a word in a translation, it
(soft-)searches for a set of positions in a source sentence where the most
relevant information is concentrated. The model then predicts a target word
*based on the context vectors associated with these source positions*
*and all the previous generated target words.*
#+end_quote

#+begin_quote
it [the model] encodes the input sentence into a sequence of vectors and chooses
a subset of these vectors adaptively while decoding the translation.'
#+end_quote

** learning to align and translate
#+begin_quote
The new architecture consists of a bidirectional RNN as an encoder (Sec. 3.2)
and a decoder that emulates searching through a source sentence during decoding
a translation (Sec.  3.1).
#+end_quote

** ref
- [[https://arxiv.org/abs/1409.0473][arxiv Neural Machine Translation by Jointly Learning to Align and Translate]]
- file:./papers/jointlyLearningToAlignAndTranslate.pdf
#+begin_src tex
@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate},
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
#+end_src


* attention? attention!
** different mechanisms
#+name: A family of attention mechanisms
#+ATTR_ORG: :width 700
[[./afamilyofattentionmechanisms.png]]

** self attention
#+begin_quote
Self-attention, also known as intra-attention, is an attention mechanism
relating different positions of a single sequence in order to compute a
representation of the same sequence. It has been shown to be very useful in
machine reading, abstractive summarization, or image description generation.
#+end_quote

#+begin_quote
In the show, attend and tell paper, attention mechanism is applied to images to
generate captions. The image is first encoded by a CNN to extract features. Then
a LSTM decoder consumes the convolution features to produce descriptive words
one by one, where the weights are learned through attention. The visualization
of the attention weights clearly demonstrates which regions of the image the
model is paying attention to so as to output a certain word.
#+end_quote
show and tell paper: [[https://arxiv.org/abs/1502.03044][{1502.03044} Show, Attend and Tell: Neural Image Caption Generation with Visu...]]

- Soft Attention: the alignment weights are learned and placed “softly” over all
  patches in the source image; essentially the same type of attention as in
  Bahdanau et al., 2015.

  + Pro: the model is smooth and differentiable.
  + Con: expensive when the source input is large.

- Hard Attention: only selects one patch of the image to attend to at a time.

  + Pro: less calculation at the inference time.
  + Con: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (Luong, et al., 2015)

** global vs local attention
#+name: global vs local attention
#+attr_org: :width 700
[[./globalvslocalattention.png]]
reference: [[https://arxiv.org/abs/1508.04025][{1508.04025} Effective Approaches to Attention-based Neural Machine Translation]]
** multi-head self attention: key value query
*important*
#+begin_quote
The transformer views the encoded representation of the input as a
set of key-value pairs, (K,V), both of dimension n (input sequence length);

in the context of NMT, both the keys and values are the encoder hidden states.
In the decoder, the previous output is compressed into a query (Q of
dimension m) and the next output is produced by mapping this query and the set
of keys and values.
#+end_quote

#+attr_org: :width 700
[[./transformerscaleddotproduct.png]]

#+attr_org: :width 700
[[./multiheadselfattention.png]]
reference: transformer paper

*important*
#+begin_quote
Rather than only computing the attention once, the multi-head mechanism runs
through the scaled dot-product attention multiple times in parallel.

The independent attention outputs are simply concatenated and linearly transformed
into the expected dimensions.

According to the paper, “multi-head attention allows the model
to jointly attend to information from different representation subspaces at
different positions. With a single attention head, averaging inhibits this.”
#+end_quote

** TODO snail
** ref
- [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
#+begin_src tex
@article{weng2018attention,
  title   = "Attention? Attention!",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2018",
  url     = "http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html"
}
#+end_src

* ressources
** arxiv links
*** Attention in natural language processing
**** ref
- [[https://arxiv.org/abs/1902.02181][{1902.02181} Attention in Natural Language Processing]]
- file:./papers/1902.02181.pdf
*** Attention is all you need
**** ref
- [[https://arxiv.org/abs/1706.03762][arxiv attention is all you need]]
- file:./papers/attentionIsAllYouNeed.pdf

*** to read and summarize
- [[https://arxiv.org/abs/1810.04805][arxiv BERT]]
- [[https://arxiv.org/abs/2009.06732][arxiv Efficient transformers a survey]]
- [[https://arxiv.org/abs/1807.03819][arxiv universal transformers]]
- [[https://arxiv.org/abs/2006.15595][{2006.15595} Rethinking Positional Encoding in Language Pre-training]]

** non academic links

- attention
  + [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
  + [[https://theaisummer.com/attention/][How Attention works in Deep Learning: understanding the attention mechanism i...]]

- positional encoding
  + [[https://kazemnejad.com/blog/transformer_architecture_positional_encoding/][Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's ...]]
  + [[https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model][nlp - What is the positional encoding in the transformer model? - Data Scienc...]]

- machine translation through alignment
  + [[https://www.tensorflow.org/tutorials/text/nmt_with_attention][Neural machine translation with attention  |  TensorFlow Core]]

- Transformer (with code)
  + [[http://nlp.seas.harvard.edu/2018/04/03/attention.html][The Annotated Transformer]]
  + [[http://vandergoten.ai/2018-09-18-attention-is-all-you-need/][Attention Is All You Need]]
  + [[https://jalammar.github.io/illustrated-transformer/][The Illustrated Transformer – Jay Alammar – Visualizing machine learning one ...]]

- videos
  + [[https://www.youtube.com/watch?v=S27pHKBEp30][LSTM is dead. Long Live Transformers! - YouTube]]

** github links

- [[https://github.com/google-research/bert][GitHub - google-research/bert: TensorFlow code and pre-trained models for BERT]]
- [[https://github.com/huggingface/transformers][GitHub - huggingface/transformers: Transformers: State-of-the-art NLP]]

** misc
- [[https://rajpurkar.github.io/SQuAD-explorer/][The Stanford Question Answering Dataset]]
- [[https://www.overleaf.com/learn/latex/bibliography_management_with_bibtex][Bibliography management with bibtex - Overleaf, Online LaTeX Editor]]
- [[https://www.overleaf.com/learn/latex/Inserting_Images][Inserting Images - Overleaf, Online LaTeX Editor]]
- [[https://github.com/hlissner/doom-emacs][GitHub - hlissner/doom-emacs: An Emacs framework for the stubborn martian hacker]]
- [[https://orgmode.org/worg/org-tutorials/][Org tutorials]]

* steps
** TODO project proposal
DEADLINE: <2021-02-05 Fri>

- file:./tex/proposaltemplate.tex
- file:./tex/proposalbibliography.bib

*** TODO understand transformer
**** TODO understand attention
**** TODO understand positional embeddings

*** TODO narrow down list of papers to reference in proposal
