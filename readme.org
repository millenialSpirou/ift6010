#+TITLE: IFT6010 project
#+author: frederic boileau
#+STARTUP: inlineimages

* attention

- [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]

*** machine translation
#+name: A family of attention mechanisms
#+ATTR_ORG: :width 700
[[./afamilyofattentionmechanisms.png]]

*** self attention
#+begin_quote
Self-attention, also known as intra-attention, is an attention mechanism
relating different positions of a single sequence in order to compute a
representation of the same sequence. It has been shown to be very useful in
machine reading, abstractive summarization, or image description generation.
#+end_quote

#+begin_quote
In the show, attend and tell paper, attention mechanism is applied to images to
generate captions. The image is first encoded by a CNN to extract features. Then
a LSTM decoder consumes the convolution features to produce descriptive words
one by one, where the weights are learned through attention. The visualization
of the attention weights clearly demonstrates which regions of the image the
model is paying attention to so as to output a certain word.
#+end_quote
show and tell paper: [[https://arxiv.org/abs/1502.03044][{1502.03044} Show, Attend and Tell: Neural Image Caption Generation with Visu...]]

- Soft Attention: the alignment weights are learned and placed “softly” over all
  patches in the source image; essentially the same type of attention as in
  Bahdanau et al., 2015.

  + Pro: the model is smooth and differentiable.
  + Con: expensive when the source input is large.

- Hard Attention: only selects one patch of the image to attend to at a time.

  + Pro: less calculation at the inference time.
  + Con: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (Luong, et al., 2015)

*** global vs local attention
#+name: global vs local attention
#+attr_org: :width 700
[[./globalvslocalattention.png]]
reference: [[https://arxiv.org/abs/1508.04025][{1508.04025} Effective Approaches to Attention-based Neural Machine Translation]]
*** multi-head self attention: key value query
#+begin_quote
The transformer views the encoded representation of the input as a
set of key-value pairs, (K,V), both of dimension n (input sequence length);

in the context of NMT, both the keys and values are the encoder hidden states.
In the decoder, the previous output is compressed into a query (Q of
dimension m) and the next output is produced by mapping this query and the set
of keys and values.
#+end_quote

#+attr_org: :width 700
[[./transformerscaleddotproduct.png]]

#+attr_org: :width 700
[[./multiheadselfattention.png]]
reference: transformer paper

* ressources
** arxiv links
*** Neural machine translation by jointly learning to align and translate (Bahdanau et all 2014)
- [[https://arxiv.org/abs/1409.0473][arxiv Neural Machine Translation by Jointly Learning to Align and Translate]]
- file:./papers/jointlyLearningToAlignAndTranslate.pdf

*Referenced a lot for attention mechanism*

``In this paper, we conjecture that the use of a fixed-length vector is a
bottleneck in improving the performance of this basic encoder–decoder architec-
ture, and propose to extend this by allowing a model to automatically
(soft-)search for parts of a source sentence that are relevant to predicting a
target word, without having to form these parts as a hard segment explicitly.''

``
Cho et al. (2014b) showed that indeed the performance of a basic
encoder–decoder deteriorates rapidly as the length of an input sentence
increases.

Each time the proposed model generates a word in a translation, it
(soft-)searches for a set of positions in a source sentence where the most
relevant information is concentrated. The model then predicts a target word
*based on the context vectors associated with these source positions*
*and all the previous generated target words.*

it [the model] encodes the input sentence into a sequence of vectors and chooses
a subset of these vectors adaptively while decoding the translation.  ''

*** Attention in natural language processing
- [[https://arxiv.org/abs/1902.02181][{1902.02181} Attention in Natural Language Processing]]
- file:./papers/1902.02181.pdf

*** Attention is all you need
- [[https://arxiv.org/abs/1706.03762][arxiv attention is all you need]]
- file:./papers/attentionIsAllYouNeed.pdf

*** to read and summarize
- [[https://arxiv.org/abs/1810.04805][arxiv BERT]]
- [[https://arxiv.org/abs/2009.06732][arxiv Efficient transformers a survey]]
- [[https://arxiv.org/abs/1807.03819][arxiv universal transformers]]
- [[https://arxiv.org/abs/2006.15595][{2006.15595} Rethinking Positional Encoding in Language Pre-training]]

** non academic links

- attention
  + [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
  + [[https://theaisummer.com/attention/][How Attention works in Deep Learning: understanding the attention mechanism i...]]

- positional encoding
  + [[https://kazemnejad.com/blog/transformer_architecture_positional_encoding/][Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's ...]]
  + [[https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model][nlp - What is the positional encoding in the transformer model? - Data Scienc...]]

- machine translation through alignment
  + [[https://www.tensorflow.org/tutorials/text/nmt_with_attention][Neural machine translation with attention  |  TensorFlow Core]]

- Transformer (with code)
  + [[http://nlp.seas.harvard.edu/2018/04/03/attention.html][The Annotated Transformer]]
  + [[http://vandergoten.ai/2018-09-18-attention-is-all-you-need/][Attention Is All You Need]]
  + [[https://jalammar.github.io/illustrated-transformer/][The Illustrated Transformer – Jay Alammar – Visualizing machine learning one ...]]

- videos
  + [[https://www.youtube.com/watch?v=S27pHKBEp30][LSTM is dead. Long Live Transformers! - YouTube]]

** github links

- [[https://github.com/google-research/bert][GitHub - google-research/bert: TensorFlow code and pre-trained models for BERT]]
- [[https://github.com/huggingface/transformers][GitHub - huggingface/transformers: Transformers: State-of-the-art NLP]]

** misc
- [[https://rajpurkar.github.io/SQuAD-explorer/][The Stanford Question Answering Dataset]]
- [[https://www.overleaf.com/learn/latex/bibliography_management_with_bibtex][Bibliography management with bibtex - Overleaf, Online LaTeX Editor]]
- [[https://www.overleaf.com/learn/latex/Inserting_Images][Inserting Images - Overleaf, Online LaTeX Editor]]
- [[https://github.com/hlissner/doom-emacs][GitHub - hlissner/doom-emacs: An Emacs framework for the stubborn martian hacker]]
- [[https://orgmode.org/worg/org-tutorials/][Org tutorials]]

* steps
** TODO project proposal
DEADLINE: <2021-02-05 Fri>

- file:./tex/proposaltemplate.tex
- file:./tex/proposalbibliography.bib

*** TODO understand transformer
**** TODO understand attention
**** TODO understand positional embeddings

*** TODO narrow down list of papers to reference in proposal
