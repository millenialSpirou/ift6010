#+TITLE: IFT6010 project
#+author: frederic boileau
#+startup: inlineimages
#+property: header-args:tex :tangle ./tex/projectbibliography.bib

* notes
** Need to google on
 - knolwedge distilliation
 - model compression
 - network pruning
** tasks for benchmarking
- Squad : [[https://rajpurkar.github.io/SQuAD-explorer/][The Stanford Question Answering Dataset]]
- GLUE: [[https://gluebenchmark.com/][GLUE Benchmark]]
- SWAG: [[https://arxiv.org/abs/1808.05326][{1808.05326} SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense...]]
- multi hop reasoning: [[https://arxiv.org/abs/1905.05460][{1905.05460} Cognitive Graph for Multi-Hop Reading Comprehension at Scale]]

* steps
** TODO project proposal
DEADLINE: <2021-02-05 Fri>
- file:./tex/proposaltemplate.tex
- file:./tex/proposalbibliography.bib

*** TODO read on knowledge distilliation
*** TODO read on model compression
*** TODO narrow down list of papers to reference in proposal

** towards midway report
*** TODO understand how to use hugging face transformers library
*** TODO understand code of transformer
*** TODO get datasets and preprocessing
*** TODO understand (fine-tuning) training for question answering

* transformers
** Neural machine translation by jointly learning to align and translate
*note: best first paper to read*
*** intro

#+begin_quote
In this paper, we conjecture that the use of a fixed-length vector is a
bottleneck in improving the performance of this basic encoder–decoder architec-
ture, and propose to extend this by allowing a model to automatically
(soft-)search for parts of a source sentence that are relevant to predicting a
target word, without having to form these parts as a hard segment explicitly.
#+end_quote

#+begin_quote
Cho et al. (2014b) showed that indeed the performance of a basic
encoder–decoder deteriorates rapidly as the length of an input sentence
increases.
#+end_quote

#+begin_quote
Each time the proposed model generates a word in a translation, it
(soft-)searches for a set of positions in a source sentence where the most
relevant information is concentrated. The model then predicts a target word
*based on the context vectors associated with these source positions*
*and all the previous generated target words.*
#+end_quote

#+begin_quote
it [the model] encodes the input sentence into a sequence of vectors and chooses
a subset of these vectors adaptively while decoding the translation.'
#+end_quote

*** learning to align and translate

#+begin_quote
The new architecture consists of a bidirectional RNN as an encoder (Sec. 3.2)
and a decoder that emulates searching through a source sentence during decoding
a translation (Sec.  3.1).
#+end_quote

#+attr_org: :width 700
[[./tex/illustrations/decodergeneraldescritptionAlignment.png]]
Personnal summary:

Each (hidden) state of the decoder RNN depends on a context vector. This context vector
is a weighted sum of the hidden states of the encoder. The weights depend on an alignment
value which scores how relevant each hidden state j is to the current output i. The weights
are normalized to form a probability distribution through a sotfmax function. All in all the
weights are the output of a simple feedforward network with the hidden states of the encoder
as input.

*** ref
- [[https://arxiv.org/abs/1409.0473][arxiv Neural Machine Translation by Jointly Learning to Align and Translate]]
- file:./papers/jointlyLearningToAlignAndTranslate.pdf
#+begin_src tex
@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate},
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
#+end_src

** The illustrated transformer
*** ref
[[https://jalammar.github.io/illustrated-transformer/][The Illustrated Transformer – Jay Alammar – Visualizing machine learning one ...]]

** Attention is all you need
*** introduction
*** self (multi-head) attention
#+attr_org: :width 800
[[./tex/illustrations/transformerSelfAttention.png]]

#+attr_org: :width 800
[[./tex/illustrations/transformerscaleddotproduct.png]]

#+begin_quote
The two most commonly used attention functions are additive attention [2], and
dot-product (multi- plicative) attention. Dot-product attention is identical to
our algorithm, except for the scaling factor of √ 1 d . Additive attention
computes the compatibility function using a feed-forward network with k a single
hidden layer. While the two are similar in theoretical complexity, dot-product
attention is much faster and more space-efficient in practice, since it can be
implemented using highly optimized matrix multiplication code.
#+end_quote

*** ref
- [[https://arxiv.org/abs/1706.03762][arxiv attention is all you need]]
- file:./papers/attentionIsAllYouNeed.pdf
#+begin_src tex
@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src

** BERT
*** introduction

#+begin_quote
There are two existing strategies for apply-
ing pre-trained language representations to down-
stream tasks: feature-based and fine-tuning. The
feature-based approach, such as ELMo (Peters
et al., 2018a), uses task-specific architectures that
include the pre-trained representations as addi-
tional features. The fine-tuning approach, such as
the Generative Pre-trained Transformer (OpenAI
GPT) (Radford et al., 2018), introduces minimal
task-specific parameters, and is trained on the
downstream tasks by simply fine-tuning all pre-
trained parameters. The two approaches share the
same objective function during pre-training, where
they use unidirectional language models to learn
general language representations.
#+end_quote

#+begin_quote
We argue that current techniques restrict the
power of the pre-trained representations, espe-
cially for the fine-tuning approaches. The ma-
jor limitation is that standard language models are
unidirectional, and this limits the choice of archi-
tectures that can be used during pre-training. For
example, in OpenAI GPT, the authors use a left-to-
right architecture, where every token can only at-
tend to previous tokens in the self-attention layers
of the Transformer (Vaswani et al., 2017). Such re-
strictions are sub-optimal for sentence-level tasks,
and could be very harmful when applying fine-
tuning based approaches to token-level tasks such
as question answering, where it is crucial to incor-
porate context from both directions.
#+end_quote

#+begin_quote
In this paper, we improve the fine-tuning based approaches by proposing BERT:
Bidirectional Encoder Representations from Transformers.  BERT alleviates the
previously mentioned unidi- rectionality constraint by using a “masked lan-
guage model” (MLM) pre-training objective, in- spired by the Cloze task (Taylor,
1953). The masked language model randomly masks some of the tokens from the
input, and the objective is to predict the original vocabulary id of the
maskedword based only on its context. Unlike left-to- right language model
pre-training, the MLM ob- jective enables the representation to fuse the left
and the right context, which allows us to pre- train a deep bidirectional
Transformer. In addi- tion to the masked language model, we also use a “next
sentence prediction” task that jointly pre- trains text-pair representations.
#+end_quote

*** implementation intro
#+begin_quote
There are two steps in our framework: pre-training and fine-tuning. During
pretraining, the model is trained on unlabeled data over different pre-training
tasks. For finetuning, the BERT model is first initialized with the
pre-trained parameters, and all of the parameters are fine-tuned using labeled
data from the downstream tasks. Each downstream task has separate fine-tuned
models, even though they are initialized with the same pretrained parameters.
#+end_quote

#+begin_quote
BERT’s model architecture is a multilayer bidirectional Transformer encoder
based on the original implementation described in Vaswani et al. (2017)
[Attention is all you need] and released in the tensor2tensor library.
#+end_quote

see attention is all you need for architecture

*** input outpout representation

#+begin_quote
To make BERT handle a variety of down-stream tasks, our input representation is
able to unambiguously represent both a single sentence and a pair of sentences
(e.g., <Question, Answer>) in one token sequence.
#+end_quote

#+begin_quote
The first token of every sequence is always a special classification token
([CLS]). The final hidden state corresponding to this token is used as the
aggregate sequence representation for classification tasks. Sentence pairs are
packed together into a single sequence. We differentiate the sentences in two
ways. First, we separate them with a special token ([SEP]). Second, we add a
learned embedding to every token indicating whether it belongs to sentence A
or sentence B.
#+end_quote

#+attr_org: :width 700
[[./tex/illustrations/bertInputRepresentation.png]]

*** pretraining
**** task 1, masked LM
#+begin_quote
In order to train a deep bidirectional representation, we simply mask some
percentage of the input tokens at random, and then predict those masked tokens.
We refer to this procedure as a “masked LM” (MLM), although it is often referred
to as a Cloze task in the literature (Taylor, 1953). In this case, the final
hidden vectors corresponding to the mask tokens are fed into an output softmax
over the vocabulary, as in a standard LM. In all of our experiments, we mask 15%
of all WordPiece tokens in each sequence at random.
#+end_quote
**** task 2 next sentence prediction
#+begin_quote
Many important downstream tasks such as Question Answering (QA) and Natural
Language Inference (NLI) are based on understanding the relationship between
two sentences, which is not di- rectly captured by language modeling. In order
to train a model that understands sentence relationships, we pre-train for a
binarized next sentence prediction task that can be trivially generated from
any monolingual corpus. Specifically, when choosing the sentences A and B for
each pre-training example, 50% of the time B is the actual next sentence that
follows A (labeled as IsNext ), and 50% of the time it is a random sentence from
the corpus (labeled as NotNext ).
#+end_quote

**** pretraining data
#+begin_quote
The pre-training procedure largely follows the existing literature on language
model pre-training. For the pre-training corpus we use the BooksCorpus (800M
words) (Zhu et al., 2015) and English Wikipedia (2,500M words).  For Wikipedia
we extract only the text passages and ignore lists, tables, and headers. It is
criti- cal to use a document-level corpus rather than a shuffled sentence-level
corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to
extract long contiguous sequences.
#+end_quote
*** fine tuning

#+begin_quote
Fine-tuning is straightforward since the self- attention mechanism in the
Transformer al- lows BERT to model many downstream tasks— whether they involve
single text or text pairs—by swapping out the appropriate inputs and outputs.
For applications involving text pairs, a common pattern is to independently
encode text pairs be- fore applying bidirectional cross attention, such as
Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention
mechanism to unify these two stages, as encoding a concatenated text pair with
self-attention effectively includes bidi- rectional cross attention between two
sentences.  For each task, we simply plug in the task- specific inputs and
outputs into BERT and fine- tune all the parameters end-to-end. At the in- put,
sentence A and sentence B from pre-training are analogous to (1) sentence pairs
in paraphras- ing, (2) hypothesis-premise pairs in entailment, (3)
question-passage pairs in question answering, and (4) a degenerate text-∅ pair
in text classification or sequence tagging. At the output, the token rep-
resentations are fed into an output layer for token- level tasks, such as
sequence tagging or question answering, and the [CLS] representation is fed into
an output layer for classification, such as entailment or sentiment analysis.
Compared to pre-training, fine-tuning is relatively inexpensive.

#+end_quote

#+begin_quote
All of the results in the paper can be replicated in at most 1 hour on a single
Cloud TPU, or *a few hours on a GPU*, starting from the exact same pre-trained
model. 7
#+end_quote

*** experiments
**** squad
#+begin_quote
in the question answering task, we represent the input question and pas- sage as
a single packed sequence, with the question using the A embedding and the
passage using the B embedding. We only introduce a start vector S ∈ R^H and an
end vector E ∈ R^H during fine-tuning. The probability of word i being the start
of the answer span is computed as a dot product between T_i and S followed by a
softmax over S·Ti all of the words in the paragraph.  The analogous formula is
used for the end of the answer span. The score of a candidate span from position
i to position j is defined as S·T_i + E·T_j , and the maximum scoring span where
j ≥ i is used as a prediction. The training objective is the sum of the
log-likelihoods of the correct start and end positions.
#+end_quote

**** squad 2
#+begin_quote
The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the
possibility that no short answer exists in the provided para- graph, making the
problem more realistic.  We use a simple approach to extend the SQuAD v1.1 BERT
model for this task. We treat questions that do not have an answer as having
an answer span with start and end at the [CLS] token. The probability space
for the start and end answer span positions is extended to include the position
of the [CLS] token.

For prediction, we compare the score of the no-answer span: s null = S·C + E·C
to the score of the best non-null span s \hat_{i,j}= max j≥i S·T_i + E·T_j .
We predict a non-null answer when s ˆ i,j > s null + τ , where the threshold τ
is selected on the dev set to maximize F1.
#+end_quote

**** swag
#+begin_quote
The Situations With Adversarial Generations (SWAG) dataset contains 113k
sentence-pair com- pletion examples that evaluate grounded commonsense
inference (Zellers et al., 2018). Given a sentence, the task is to choose the
most plausible continuation among four choices.  When fine-tuning on the SWAG
dataset, we construct four input sequences, each containing the concatenation of
the given sentence (sentence A) and a possible continuation (sentence B). The
only task-specific parameters introduced is a vector whose dot product with
the [CLS] token representation C denotes a score for each choice which is
normalized with a softmax layer.
#+end_quote

**** effect of model size

#+begin_quote
we believe that this is the first work to demonstrate convincingly
that scaling to extreme model sizes also leads to large improvements on very
small scale tasks, provided that the model has been sufficiently pre-trained.
#+end_quote
*** conclusion
#+begin_quote
Recent empirical improvements due to transfer learning with language models have
demonstrated that rich, unsupervised pre-training is an integral part of many
language understanding systems. In particular, these results enable even
low-resource tasks to benefit from deep unidirectional architectures. Our major
contribution is further generalizing these findings to deep bidirectional
architectures, allowing the same pre-trained model to successfully tackle a
broad set of NLP tasks.
#+end_quote


*** ref
- [[https://arxiv.org/abs/1810.04805][{1810.04805} BERT: Pre-training of Deep Bidirectional Transformers for Langua...]]
- file:./papers/1810.04805.pdf

#+begin_src tex
@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src

* model compression
** TinyBert
*** abstract
#+begin_quote
Language model pre-training, such as BERT, has significantly improved the
performances of many natural language processing tasks.  However, pre-trained
language models are usually computationally expensive, so it is difficult to
efficiently execute them on resource-restricted devices. To accelerate
inference and reduce model size while maintaining accuracy, we first propose a
novel Transformer distillation method that is specially designed for
knowledge distillation (KD) of the Transformer-based models.
#+end_quote

#+begin_quote
By leveraging this new KD method, the plenty of knowledge encoded in a large
“teacher” BERT can be effectively transferred to a small “student” TinyBERT.
Then, we introduce a new *two-stage learning framework* for TinyBERT, which per-
forms Transformer distillation at both the pre- training and task-specific
learning stages. This framework ensures that TinyBERT can capture the
general-domain as well as the task-specific knowledge in BERT.
#+end_quote

#+begin_quote
TinyBERT4 with 4 layers is empirically effective and achieves more than
96.8% the performance of its teacher BERT BASE on GLUE benchmark, while being
7.5x smaller and 9.4x faster on inference.

TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines
on BERT distillation, with only ∼28% parameters and ∼31% inference time of them.
Moreover, TinyBERT 6 with 6 layers performs on-par with its teacher BERT BASE .
#+end_quote
*** introduction

PLMs based on the transformer architecture such as BERT, XLNet, RoBERTa, etc
have been sucessful at many NLP tasks including the GLUE benchmark.
#+begin_quote
Pre-trained language models (PLMs), such as BERT (Devlin et al., 2019), XLNet
(Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), T5
(Raffel et al., 2019) and ELECTRA (Clark et al., 2020), have achieved great
success in many NLP tasks (e.g., the GLUE benchmark (Wang et al., 2018) and the
challenging multi-hop reasoning task (Ding et al., 2019)).
#+end_quote

#+begin_quote
However, PLMs usually have a large number of parameters and take long infer-
ence time, which are difficult to be deployed on edge devices such as mobile
phones. Recent studies (Kovaleva et al., 2019; Michel et al., 2019; Voita et
al., 2019) demonstrate that there is redundancy in PLMs. Therefore, it is
crucial and feasible to reduce the computational overhead and model storage of
PLMs while retaining their performances.
#+end_quote

model compression techniques:
- quantization
- weights pruning
- Knowldge distillation
#+begin_quote
There have been many model compression techniques (Han et al., 2016) proposed
to accelerate deep model inference and reduce model size while maintaining
accuracy. The most commonly used techniques include quantization (Gong et al.,
2014), weights pruning (Han et al., 2015), and knowledge distillation (KD)
(Romero et al., 2014). In this paper, we focus on knowledge distillation, an
idea originated from Hinton et al. (2015), in a teacher-student framework. KD
aims to transfer the knowledge embedded in a large teacher net- work to a small
student network where the student network is trained to reproduce the behaviors
of the teacher network. Based on the framework, we pro- pose a novel
distillation method specifically for the Transformer-based models (Vaswani et
al., 2017), and use BERT as an example to investigate the method for large-scale
PLMs.
#+end_quote

#+begin_quote
it is required to design an effective KD strategy for both training stages.
#+end_quote
*** related work
- PLM compression techniques:
  + low rank approximation
  + weight sharing
  + knowledge distillation
  + pruning
  + quantization
#+begin_quote
Pre-trained Language Models Compression Generally, pre-trained language models
(PLMs) can be compressed by low-rank approximation (Ma et al., 2019; Lan et al.,
2020), weight sharing (De- hghani et al., 2019; Lan et al., 2020), knowledge
distillation (Tang et al., 2019; Sanh et al., 2019; Turc et al., 2019; Sun et
al., 2020; Liu et al., 2020; Wang et al., 2020), pruning (Cui et al., 2019; Mc-
Carley, 2019; F. et al., 2020; Elbayad et al., 2020; Gordon et al., 2020; Hou et
al., 2020) or quantiza- tion (Shen et al., 2019; Zafrir et al., 2019).
#+end_quote

- Pretraining lite PLMS
  + ALBERT
  + ELECTRA
#+begin_quote
Pretraining Lite PLMs Other related works aim at directly pretraining lite
PLMs. Turc et al. (2019) pre-trained 24 miniature BERT models and show that
pre-training remains important in the context of smaller architectures, and
fine-tuning pretrained compact models can be competitive. ALBERT (Lan et
al., 2020) incorporates embedding factorization and cross-layer parameter
sharing to reduce model parameters. Since ALBERT does not reduce hidden size or
layers of transformer block, it still has large amount of computations. Another
concurrent work, ELECTRA (Clark et al., 2020) proposes a sample-efficient task
called replaced to- ken detection to accelerate pre-training, and it also
presents a 12-layer ELECTRA small that has com- parable performance with
TinyBERT 4 . Differentfrom these small PLMs, TinyBERT 4 is a 4-layer model which
can achieve more speedup.
#+end_quote
*** conclusion
#+begin_quote
In future work, we would study how to effectively
transfer the knowledge from wider and deeper
teachers (e.g., BERT LARGE ) to student TinyBERT.
Combining distillation with quantization/pruning
would be another promising direction to further
compress the pre-trained language models.
#+end_quote

*** ref
[[https://arxiv.org/abs/1909.10351][{1909.10351} TinyBERT: Distilling BERT for Natural Language Understanding]]
file:./papers/1909.10351.pdf
#+begin_src tex
@article{DBLP:journals/corr/abs-1909-10351,
  author    = {Xiaoqi Jiao and
               Yichun Yin and
               Lifeng Shang and
               Xin Jiang and
               Xiao Chen and
               Linlin Li and
               Fang Wang and
               Qun Liu},
  title     = {TinyBERT: Distilling {BERT} for Natural Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1909.10351},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.10351},
  archivePrefix = {arXiv},
  eprint    = {1909.10351},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-10351.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src
** knowledge distillation
*** introduction
#+begin_quote
In large-scale machine learning, we typically use very similar models for the
training stage and the deployment stage despite their very different
requirements: For tasks like speech and object recognition, training must
extract structure from very large, highly redundant datasets but it does not
need to operate in real time and it can use a huge amount of computation.
Deployment to a large number of users, however, has much more stringent
requirements on latency and computational resources.
#+end_quote

#+begin_quote
A conceptual block that may have prevented more investigation of this very
promising approach is that we tend to identify the knowledge in a trained model
with the learned parameter values and this makes it hard to see how we can
change the form of the model but keep the same knowledge.  A more abstract view
of the knowledge, that frees it from any particular instantiation, is that it is
a learned mapping from input vectors to output vectors.
#+end_quote

#+begin_quote
It is generally accepted that the objective function used for training should
reflect the true objective of the user as closely as possible. Despite this,
models are usually trained to optimize performance on the training data when the
real objective is to generalize well to new data. It would clearly be better to
train models to generalize well, but this requires information about the correct
way to generalize and this information is not normally available. When we are
distilling the knowledge from a large model into a small one, however, we can
train the small model to generalize in the same way as the large model. If the
cumbersome model generalizes well because, for example, it is the average of a
large ensemble of different models, a small model trained to generalize in the
same way will typically do much better on test data than a small model that is
trained in the normal way on the same training set as was used to train the
ensemble.
#+end_quote


*** ref
file:./papers/1503.02531.pdf
[[https://arxiv.org/abs/1503.02531][{1503.02531} Distilling the Knowledge in a Neural Network]]
#+begin_src tex
@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network},
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
#+end_src

* Code relevant links
- Transformer
  + [[http://nlp.seas.harvard.edu/2018/04/03/attention.html][The Annotated Transformer]]
  + [[https://becominghuman.ai/attention-is-all-you-need-16bf481d8b5c][Attention is all you need. An explanation about transformer | by Pierrick RUG...]]
  + [[http://vandergoten.ai/2018-09-18-attention-is-all-you-need/][Attention Is All You Need]]

- pretraining bert
  + [[https://d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html][14.9. The Dataset for Pretraining BERT — Dive into Deep Learning 0.16.1 docum...]]
  + [[https://github.com/google-research/bert/issues/750][google-research/bert#750 Creating training set from Wikipedia data?]]
  + [[https://towardsdatascience.com/preparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a][Preparing the data for Transformer pre-training — a write-up | by Steven van ...]]
  + [[https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67][Pre-processing a Wikipedia dump for NLP model training — a write-up | by Stev...]]

* Low ressource transformers
** Efficient transformers: a survey
*** introduction
#+begin_quote
There has been such a surge of Transformer model variants proposed recently, that
researchers and practitioners alike may find it challenging to keep pace with the rate of
innovation. As of this writing (circa August 2020), there have been nearly a dozen new
efficiency-focused models proposed in just the past 6 months.
#+end_quote

#+begin_quote
The self-attention mechanism is a key defining characteristic of Transformer models.
The mechanism can be viewed as a graph-like inductive bias that connects all tokens in
a sequence with a relevance-based pooling operation. A well-known concern with self-
attention is the *quadratic time and memory complexity*, which can hinder model scalability
in many settings. There has been an overwhelming influx of model variants proposed
recently that address this problem. We hereinafter name this class of models “efficient
Transformers”.
#+end_quote

*** background on transformers
#+attr_org: :width 600
[[./tex/illustrations/transformerMathSummary.png]]

#+begin_quote
It is important to note the differences in the mode of usage of the Transformer
block.  Transformers can primarily be used in three ways, namely: (1)
encoder-only (e.g., for classification), (2) decoder-only (e.g., for language
modeling), and (3) encoder-decoder (e.g., for machine translation). In
encoder-decoder mode, there are usually multiple multi-headed self-attention
modules, including a standard self-attention in both the encoder and the
decoder, *along with an encoder-decoder cross-attention that allows the decoder
to utilize information from the encoder.* This influences the design of the
self-attention mechanism.

In the encoder mode, there is no restriction or constraint that the
self-attention mechanism has to be causal, i.e., dependent solely on the present
and past tokens.

In the encoder-decoder setting, the encoder and encoder-decoder cross attention
can afford to be non-causal but the *decoder self-attention must be causal.*
#+end_quote

*** a survey of efficient transformers
#+attr_org: :width 800
[[./tex/illustrations/taxonomyEfficientTransformers.png]]

#+begin_quote
The primary goal of most of these models, with the exception of those based on
segment-based recurrence, is to approximate the quadratic- cost attention
matrix. Each method applies some notion of sparsity to the otherwise dense
attention mechanism.
#+end_quote

*** ref
- [[https://arxiv.org/abs/2009.06732][{2009.06732} Efficient Transformers: A Survey]]
- file:./papers/2009.06732.pdf

#+begin_src tex
@misc{tay2020efficient,
      title={Efficient Transformers: A Survey},
      author={Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
      year={2020},
      eprint={2009.06732},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
#+end_src

** Reformer
*** ref
[[https://openreview.net/forum?id=rkgNKkHtvB][Reformer: The Efficient Transformer | OpenReview]]
** ALBERT
*** ref
[[https://openreview.net/forum?id=H1eA7AEtvS][ALBERT: A Lite BERT for Self-supervised Learning of Language Representations ...]]
** Sparse Transformers
*** ref
[[https://openai.com/blog/sparse-transformer/][Generative Modeling with Sparse Transformers]]
** Simple Recurrent units
*** ref
[[https://arxiv.org/abs/1709.02755][{1709.02755} Simple Recurrent Units for Highly Parallelizable Recurrence]]

** do we really need model compression
*** ref
[[http://mitchgordon.me/machine/learning/2020/01/13/do-we-really-need-model-compression.html][Do We Really Need Model Compression? | Mitchell A. Gordon]]
#+begin_src tex
@misc{gordon_2019,
    author = "Mitchell A. Gordon",
    title = "Do We Really Need Model Compression?",
    year = "2020",
    howpublished="http://mitchgordon.me/machine/learning/2020/01/13/do-we-really-need-model-compression.html",
}
#+end_src
** Rethinking the value of network pruning
*** ref
[[https://openreview.net/forum?id=rJlnB3C5Ym][Rethinking the Value of Network Pruning | OpenReview]]
file:./papers/rethinking_the_value_of_network_pruning.pdf
#+begin_src tex
@inproceedings{
    liu2018rethinking,
    title={Rethinking the Value of Network Pruning},
    author={Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=rJlnB3C5Ym},
}
#+end_src

* Annotated articles not directly relevant to proposal
** Attention in Natural Language Processing
*** intro
Four dimensions to the taxonomy:
- the representation of the input,
- the compatibility function,
- the distribution function,
- the multiplicity of the input and/or output.

*** attention models
#+attr_org: :width 700
[[./tex/illustrations/coreAttentionModel.png]]

#+attr_org: :width 800
[[./tex/illustrations/generaelAttentionModel.png]]

*** use of attention
#+begin_quote
Attention enables us to estimate the relevance of the input
elements as well as to combine said elements into a com-
pact representation—the context vector—that condenses the
characteristics of the most relevant elements. Because the
context vector is smaller than the original input, it requires
fewer computational resources to be processed at later stages,
yielding a computational gain.
#+end_quote

#+begin_quote
When the generation of a text sequence is required, as in
machine translation, attention enables us to make use of a
dynamic representation of the input sequence, whereby the
whole input does not have to be encoded into a single vector.
At each time step, the encoding is tailored according to the
task, and in particular, q represents an embedding of the
previous state of the decoder. More generally, the possibility to
perform attention with respect to a query q allows us to create
representations of the input that depend on the task context,
creating specialized embeddings. This is particularly useful in
tasks, such as sentiment analysis and information extraction.
#+end_quote

#+begin_quote
Since attention can create contextual representations of an
element, it can also be used to build sequence-to-sequence
annotators, without resorting to RNNs or convolutional neural
networks (CNNs), as suggested by Vaswani et al. [36],
who rely on an attention mechanism to obtain a whole
encoder/decoder architecture.
#+end_quote

#+begin_quote
Attention can also be used as a tool for selecting specific
words. This could be the case, for example, in dependence
parsing [97] and in cloze question-answering tasks [66], [70].
In the former case, attention can be applied to a sentence in
order to predict dependences. In the latter, attention can be
applied to a textual document or to a vocabulary to perform a
classification among the words.
#+end_quote

#+begin_quote
Finally, attention can come in handy when multiple inter-
acting input sequences have to be considered in combination.
In tasks such as question answering, where the input consists
of two textual sequences—for instance, the question and
the document or the question and the possible answers—an
input encoding can be obtained by considering the mutual
interactions between the elements of such sequences, rather
than by applying a more rigid a priori defined model.
#+end_quote

*** taxonomy for attention models
#+begin_quote
In NLP-related tasks, generally, K and V are representations
of parts of documents, such as sequences of characters, words,
or sentences. These components are usually embedded into
continuous vector representations and then processed through
key/value annotation functions (called kaf and vaf in Fig. 4),
so as to obtain a hidden representation resulting in K and V .
Typical annotation functions are RNN layers such as gated
recurrent units (GRUs), long short-term memory networks
(LSTMs), and CNNs. In this way, k i and v i represent an input
element relative to its local context.
#+end_quote

#+begin_quote
We made a distinction between two input sources: the input sequence, represented
by K and V , and the query, represented by q. However, some architectures
compute attention only based on the input sequence. These architectures are
known as self-attentive or intraattentive mod- els.
#+end_quote


#+begin_quote
The commonest one amounts to the application of multiple steps of attention to a
vector K , using the elements k t of the same vector as query at each step [18],
[36]. At each step, the weights a i t represent the relevance of k i with
respect to k t , yielding d K separate context embeddings, c t , one per key.
#+end_quote

#+begin_quote
Attention could thus be used as a sequence-to-sequence model, as an alternative
to CNNs or RNNs (see Fig. 5). In this way, each element of the new sequence may
be influenced by elements of the whole input, incorporating contextual
information without any locality boundaries. This is especially interesting
since it could overcome a well-known shortcoming of RNNs: their limited ability
of modeling long-range dependences [140]. For each element k t , the resulting
distribution of the weights a t should give more emphasis to words that strongly
relate to k t . The analysis of these distributions will, therefore, provide
information regarding the relationship between the elements inside the sequence.
Modern text-sequence generation systems often rely on this approach
#+end_quote

*** hiarchical attention
#+begin_quote
Hierarchical-Input Architectures: In some tasks, portions
of input data can be meaningfully grouped together into higher
level structures, where hierarchical-input attention models can
be exploited to subsequently apply multiple attention modules
at different levels of the composition, as shown in Fig. 6.
Consider, for instance, data naturally associated with a
two-level semantic structure, such as characters (the “micro”
elements) forming words (the “macro” elements) or words
forming sentences. Attention can be first applied to the rep-
resentations of micro elements k i , so as to build aggregate
representations k j of the macro elements, such as context
vectors. Attention could then be applied again to the sequence
of macroelement embeddings, in order to compute an embed-
ding for the whole document D. With this model, attention
first highlights the most relevant micro elements within each
macro element and, then, the most relevant macro elements in
the document. For instance, Yang et al. [52] applied attention
first at word level, for each sentence in turn, to compute
sentence embeddings. Then, they applied attention again on
the sentence embeddings to obtain a document representation.
7
With reference to the model introduced in Section II, embed-
dings are computed for each sentence in D, and then, all
such embeddings are used together as keys K to compute the
document-level weights a and eventually D’s context vector c.
The hierarchy can be extended further. For instance, Wu et al.
[141] added another layer on top, applying attention also at
the document level.
If representations for both micro- and macro-level elements
are available, one can compute attention on one level and
then exploit the result as a key or query to compute atten-
tion on the other, yielding two different microrepresenta-
tion/macrorepresentation of D. In this way, attention enables
us to identify the most relevant elements for the task at both
levels. The attention-via-attention model by Zhao and Zhang
[43] defines a hierarchy with characters at the micro level and
words at the macro level. Both characters and words act as
keys. Attention is first computed on word embeddings K W ,
thus obtaining a document representation in the form of a
context vector c W , which in turn acts as a query q to guide the
application of character-level attention to the keys (character
embeddings) K C , yielding a context vector c for D.
Ma et al. [113] identified a single “target” macro-object T
as a set of words, which do not necessarily have to form a
sequence in D, and then used such a macro-object as keys,
K T . The context vector c T produced by a first application of
the attention mechanism on K T is then used as query q in
a second application of the attention mechanism over D, with
the keys being the document’s word embeddings K W .
#+end_quote

*** ref
- [[https://arxiv.org/abs/1902.02181][{1902.02181} Attention in Natural Language Processing]]
- file:~/udem/nlp/project/papers/1902.02181.pdf
#+begin_src tex
@article{DBLP:journals/corr/abs-1902-02181,
  author    = {Andrea Galassi and
               Marco Lippi and
               Paolo Torroni},
  title     = {Attention, please! {A} Critical Review of Neural Attention Models
               in Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1902.02181},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.02181},
  archivePrefix = {arXiv},
  eprint    = {1902.02181},
  timestamp = {Wed, 25 Sep 2019 17:52:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-02181.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src

** attention? attention!
*Note: difficult for now might point to fancy improvements to consider in the future*
*** different mechanisms
#+name: A family of attention mechanisms
#+ATTR_ORG: :width 700
[[./tex/illustrations/afamilyofattentionmechanisms.png]]

*** self attention
#+begin_quote
Self-attention, also known as intra-attention, is an attention mechanism
relating different positions of a single sequence in order to compute a
representation of the same sequence. It has been shown to be very useful in
machine reading, abstractive summarization, or image description generation.
#+end_quote

#+begin_quote
In the show, attend and tell paper, attention mechanism is applied to images to
generate captions. The image is first encoded by a CNN to extract features. Then
a LSTM decoder consumes the convolution features to produce descriptive words
one by one, where the weights are learned through attention. The visualization
of the attention weights clearly demonstrates which regions of the image the
model is paying attention to so as to output a certain word.
#+end_quote
show and tell paper: [[https://arxiv.org/abs/1502.03044][{1502.03044} Show, Attend and Tell: Neural Image Caption Generation with Visu...]]

- Soft Attention: the alignment weights are learned and placed “softly” over all
  patches in the source image; essentially the same type of attention as in
  Bahdanau et al., 2015.

  + Pro: the model is smooth and differentiable.
  + Con: expensive when the source input is large.

- Hard Attention: only selects one patch of the image to attend to at a time.

  + Pro: less calculation at the inference time.
  + Con: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (Luong, et al., 2015)

*** global vs local attention
#+name: global vs local attention
#+attr_org: :width 700
[[./tex/illustrations/globalvslocalattention.png]]

reference: [[https://arxiv.org/abs/1508.04025][{1508.04025} Effective Approaches to Attention-based Neural Machine Translation]]
*** multi-head self attention: key value query
*important*
#+begin_quote
The transformer views the encoded representation of the input as a
set of key-value pairs, (K,V), both of dimension n (input sequence length);

in the context of NMT, both the keys and values are the encoder hidden states.
In the decoder, the previous output is compressed into a query (Q of
dimension m) and the next output is produced by mapping this query and the set
of keys and values.
#+end_quote

#+attr_org: :width 700
[[./tex/illustrations/transformerscaleddotproduct.png]]

#+attr_org: :width 700
[[./tex/illustrations/multiheadselfattention.png]]
reference: transformer paper

*important*
#+begin_quote
Rather than only computing the attention once, the multi-head mechanism runs
through the scaled dot-product attention multiple times in parallel.

The independent attention outputs are simply concatenated and linearly transformed
into the expected dimensions.

According to the paper, “multi-head attention allows the model
to jointly attend to information from different representation subspaces at
different positions. With a single attention head, averaging inhibits this.”
#+end_quote

*** TODO snail
*** ref
- [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
#+begin_src tex
@article{weng2018attention,
  title   = "Attention? Attention!",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2018",
  url     = "http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html"
}
#+end_src

* other ressources
** github links
- [[https://github.com/google-research/bert][GitHub - google-research/bert: TensorFlow code and pre-trained models for BERT]]
- [[https://github.com/huggingface/transformers][GitHub - huggingface/transformers: Transformers: State-of-the-art NLP]]
** misc arxiv links
- [[https://arxiv.org/abs/2009.06732][arxiv Efficient transformers a survey]]
- [[https://arxiv.org/abs/1807.03819][arxiv universal transformers]]
- [[https://arxiv.org/abs/2006.15595][{2006.15595} Rethinking Positional Encoding in Language Pre-training]]
** non academic links
- easier explanations of transformer
  + [[https://jalammar.github.io/illustrated-transformer/][The Illustrated Transformer – Jay Alammar – Visualizing machine learning one ...]]
- attention
  + [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
  + [[https://theaisummer.com/attention/][How Attention works in Deep Learning: understanding the attention mechanism i...]]
- positional encoding
  + [[https://kazemnejad.com/blog/transformer_architecture_positional_encoding/][Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's ...]]
  + [[https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model][nlp - What is the positional encoding in the transformer model? - Data Scienc...]]
- machine translation through alignment
  + [[https://www.tensorflow.org/tutorials/text/nmt_with_attention][Neural machine translation with attention  |  TensorFlow Core]]
- videos
  + [[https://www.youtube.com/watch?v=rBCqOTEfxvg][Attention is all you need; Attentional Neural Network Models | Łukasz Kaiser ...]]
  + [[https://www.youtube.com/watch?v=-QH8fRhqFHM][The Narrated Transformer Language Model - YouTube]]
  + [[https://www.youtube.com/watch?v=S27pHKBEp30][LSTM is dead. Long Live Transformers! - YouTube]]

** misc

- [[https://github.com/hlissner/doom-emacs][GitHub - hlissner/doom-emacs: An Emacs framework for the stubborn martian hacker]]
- [[https://orgmode.org/worg/org-tutorials/][Org tutorials]]
- [[https://org-babel.readthedocs.io/en/latest/header-args/][Header arguments - Org Babel reference card]]

- [[https://www.overleaf.com/3324588169zyyzyysrrtmw][Overleaf, Online LaTeX Editor]]
- [[https://www.overleaf.com/learn/latex/bibliography_management_with_bibtex][Bibliography management with bibtex - Overleaf, Online LaTeX Editor]]
- [[https://www.overleaf.com/learn/latex/Inserting_Images][Inserting Images - Overleaf, Online LaTeX Editor]]
