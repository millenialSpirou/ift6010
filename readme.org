#+TITLE: IFT6010 project
#+author: frederic boileau
#+STARTUP: inlineimages

* notes
- *Albert, a light version of BERT already surpasses humans in SquAd*

- *Finetuning, not even pretraining BERT, can take hours with a solid gpu*

- The first paper on attention: Jointly learning to align and translate
  still uses RNN, but attention mechanism from the decoder to the encoder
  hiddens states. It's a good introduction to the topic and has *historical value*

- Transformer is huge and hard to grasp it all, the hardest and most important
  part is the *multi headed attention*. I'm still trying to understand it

- I propose we do simple albert for implementation and Lit reviews
  on more advanced topics. I predict there is gonna be a gap between
  SOA stuff we can implement on a specific task like squad and papers
  the prof will be interested in reading summaries of. So my idea is
  to do both in parallel. If we're lucky we have time to implement
  the more fancy stuff. If it's interesting enough the prof might give
  us an extension.

- Ideas for the more fancy stuff than squad with albert in order of /perceived/ difficulty:

  + Squad is solved but there is always more room to *optimize training time*
    More efficient version of transformers could be implemented or reviewed,
    I want to read -[[https://arxiv.org/abs/2009.06732][arxiv Efficient transformers a survey]] when I understand
    transformer.

  + tackling something more difficult than Squad might be interesting since
    Albert beats humans I think the prof would consider it a /solved/ problem.

  + The paper Attention in Natural Language Processing provides a classification
    of different attention mechanisms by four criteria. It might be interesting
    to investigate the different types of attentions, for example *hierarchical
    attention*

  + A lit review on SNAIL and meta-learning might be interesting, see attention? attention!


* Neural machine translation by jointly learning to align and translate
*note: best first paper to read*
** intro

#+begin_quote
In this paper, we conjecture that the use of a fixed-length vector is a
bottleneck in improving the performance of this basic encoder–decoder architec-
ture, and propose to extend this by allowing a model to automatically
(soft-)search for parts of a source sentence that are relevant to predicting a
target word, without having to form these parts as a hard segment explicitly.
#+end_quote

#+begin_quote
Cho et al. (2014b) showed that indeed the performance of a basic
encoder–decoder deteriorates rapidly as the length of an input sentence
increases.
#+end_quote

#+begin_quote
Each time the proposed model generates a word in a translation, it
(soft-)searches for a set of positions in a source sentence where the most
relevant information is concentrated. The model then predicts a target word
*based on the context vectors associated with these source positions*
*and all the previous generated target words.*
#+end_quote

#+begin_quote
it [the model] encodes the input sentence into a sequence of vectors and chooses
a subset of these vectors adaptively while decoding the translation.'
#+end_quote

** learning to align and translate

#+begin_quote
The new architecture consists of a bidirectional RNN as an encoder (Sec. 3.2)
and a decoder that emulates searching through a source sentence during decoding
a translation (Sec.  3.1).
#+end_quote

#+attr_org: :width 700
[[./tex/illustrations/decodergeneraldescritptionAlignment.png]]
Personnal summary:

Each (hidden) state of the decoder RNN depends on a context vector. This context vector
is a weighted sum of the hidden states of the encoder. The weights depend on an alignment
value which scores how relevant each hidden state j is to the current output i. The weights
are normalized to form a probability distribution through a sotfmax function. All in all the
weights are the output of a simple feedforward network with the hidden states of the encoder
as input.

** ref
- [[https://arxiv.org/abs/1409.0473][arxiv Neural Machine Translation by Jointly Learning to Align and Translate]]
- file:./papers/jointlyLearningToAlignAndTranslate.pdf
#+begin_src tex
@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate},
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
#+end_src


* The illustrated transformer
** ref
[[https://jalammar.github.io/illustrated-transformer/][The Illustrated Transformer – Jay Alammar – Visualizing machine learning one ...]]


* Attention is all you need
** introduction
** self (multi-head) attention
#+attr_org: :width 800
[[./tex/illustrations/transformerSelfAttention.png]]

#+attr_org: :width 800
[[./tex/illustrations/transformerscaleddotproduct.png]]

#+begin_quote
The two most commonly used attention functions are additive attention [2], and
dot-product (multi- plicative) attention. Dot-product attention is identical to
our algorithm, except for the scaling factor of √ 1 d . Additive attention
computes the compatibility function using a feed-forward network with k a single
hidden layer. While the two are similar in theoretical complexity, dot-product
attention is much faster and more space-efficient in practice, since it can be
implemented using highly optimized matrix multiplication code.
#+end_quote

** ref
- [[https://arxiv.org/abs/1706.03762][arxiv attention is all you need]]
- file:./papers/attentionIsAllYouNeed.pdf
#+begin_src tex
@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src



* BERT
** introduction

#+begin_quote
There are two existing strategies for apply-
ing pre-trained language representations to down-
stream tasks: feature-based and fine-tuning. The
feature-based approach, such as ELMo (Peters
et al., 2018a), uses task-specific architectures that
include the pre-trained representations as addi-
tional features. The fine-tuning approach, such as
the Generative Pre-trained Transformer (OpenAI
GPT) (Radford et al., 2018), introduces minimal
task-specific parameters, and is trained on the
downstream tasks by simply fine-tuning all pre-
trained parameters. The two approaches share the
same objective function during pre-training, where
they use unidirectional language models to learn
general language representations.
#+end_quote

#+begin_quote
We argue that current techniques restrict the
power of the pre-trained representations, espe-
cially for the fine-tuning approaches. The ma-
jor limitation is that standard language models are
unidirectional, and this limits the choice of archi-
tectures that can be used during pre-training. For
example, in OpenAI GPT, the authors use a left-to-
right architecture, where every token can only at-
tend to previous tokens in the self-attention layers
of the Transformer (Vaswani et al., 2017). Such re-
strictions are sub-optimal for sentence-level tasks,
and could be very harmful when applying fine-
tuning based approaches to token-level tasks such
as question answering, where it is crucial to incor-
porate context from both directions.
#+end_quote

#+begin_quote
In this paper, we improve the fine-tuning based approaches by proposing BERT:
Bidirectional Encoder Representations from Transformers.  BERT alleviates the
previously mentioned unidi- rectionality constraint by using a “masked lan-
guage model” (MLM) pre-training objective, in- spired by the Cloze task (Taylor,
1953). The masked language model randomly masks some of the tokens from the
input, and the objective is to predict the original vocabulary id of the
maskedword based only on its context. Unlike left-to- right language model
pre-training, the MLM ob- jective enables the representation to fuse the left
and the right context, which allows us to pre- train a deep bidirectional
Transformer. In addi- tion to the masked language model, we also use a “next
sentence prediction” task that jointly pre- trains text-pair representations.
#+end_quote

** implementation intro
#+begin_quote
There are two steps in our framework: pre-training and fine-tuning. During
pretraining, the model is trained on unlabeled data over different pre-training
tasks. For finetuning, the BERT model is first initialized with the
pre-trained parameters, and all of the parameters are fine-tuned using labeled
data from the downstream tasks. Each downstream task has separate fine-tuned
models, even though they are initialized with the same pretrained parameters.
#+end_quote

#+begin_quote
BERT’s model architecture is a multilayer bidirectional Transformer encoder
based on the original implementation described in Vaswani et al. (2017)
[Attention is all you need] and released in the tensor2tensor library.
#+end_quote

see attention is all you need for architecture

** input outpout representation

#+begin_quote
To make BERT handle a variety of down-stream tasks, our input representation is
able to unambiguously represent both a single sentence and a pair of sentences
(e.g., <Question, Answer>) in one token sequence.
#+end_quote

#+begin_quote
The first token of every sequence is always a special classification token
([CLS]). The final hidden state corresponding to this token is used as the
aggregate sequence representation for classification tasks. Sentence pairs are
packed together into a single sequence. We differentiate the sentences in two
ways. First, we separate them with a special token ([SEP]). Second, we add a
learned embedding to every token indicating whether it belongs to sentence A
or sentence B.
#+end_quote

#+attr_org: :width 700
[[./tex/illustrations/bertInputRepresentation.png]]

** pretraining
*** task 1, masked LM
#+begin_quote
In order to train a deep bidirectional representation, we simply mask some
percentage of the input tokens at random, and then predict those masked tokens.
We refer to this procedure as a “masked LM” (MLM), although it is often referred
to as a Cloze task in the literature (Taylor, 1953). In this case, the final
hidden vectors corresponding to the mask tokens are fed into an output softmax
over the vocabulary, as in a standard LM. In all of our experiments, we mask 15%
of all WordPiece tokens in each sequence at random.
#+end_quote
*** task 2 next sentence prediction
#+begin_quote
Many important downstream tasks such as Question Answering (QA) and Natural
Language Inference (NLI) are based on understanding the relationship between
two sentences, which is not di- rectly captured by language modeling. In order
to train a model that understands sentence relationships, we pre-train for a
binarized next sentence prediction task that can be trivially generated from
any monolingual corpus. Specifically, when choosing the sentences A and B for
each pre-training example, 50% of the time B is the actual next sentence that
follows A (labeled as IsNext ), and 50% of the time it is a random sentence from
the corpus (labeled as NotNext ).
#+end_quote

*** pretraining data
#+begin_quote
The pre-training procedure largely follows the existing literature on language
model pre-training. For the pre-training corpus we use the BooksCorpus (800M
words) (Zhu et al., 2015) and English Wikipedia (2,500M words).  For Wikipedia
we extract only the text passages and ignore lists, tables, and headers. It is
criti- cal to use a document-level corpus rather than a shuffled sentence-level
corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to
extract long contiguous sequences.
#+end_quote
** fine tuning

#+begin_quote
Fine-tuning is straightforward since the self- attention mechanism in the
Transformer al- lows BERT to model many downstream tasks— whether they involve
single text or text pairs—by swapping out the appropriate inputs and outputs.
For applications involving text pairs, a common pattern is to independently
encode text pairs be- fore applying bidirectional cross attention, such as
Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention
mechanism to unify these two stages, as encoding a concatenated text pair with
self-attention effectively includes bidi- rectional cross attention between two
sentences.  For each task, we simply plug in the task- specific inputs and
outputs into BERT and fine- tune all the parameters end-to-end. At the in- put,
sentence A and sentence B from pre-training are analogous to (1) sentence pairs
in paraphras- ing, (2) hypothesis-premise pairs in entailment, (3)
question-passage pairs in question answering, and (4) a degenerate text-∅ pair
in text classification or sequence tagging. At the output, the token rep-
resentations are fed into an output layer for token- level tasks, such as
sequence tagging or question answering, and the [CLS] representation is fed into
an output layer for classification, such as entailment or sentiment analysis.
Compared to pre-training, fine-tuning is relatively inexpensive.

#+end_quote

#+begin_quote
All of the results in the paper can be replicated in at most 1 hour on a single
Cloud TPU, or *a few hours on a GPU*, starting from the exact same pre-trained
model. 7
#+end_quote

** ref
- [[https://arxiv.org/abs/1810.04805][{1810.04805} BERT: Pre-training of Deep Bidirectional Transformers for Langua...]]
- file:~/udem/nlp/project/papers/1810.04805.pdf

#+begin_src tex
@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src


* transformer code
** ref
- pretraining bert
  + [[https://github.com/google-research/bert/issues/750][google-research/bert#750 Creating training set from Wikipedia data?]]
  + [[https://towardsdatascience.com/preparing-the-data-for-transformer-pre-training-a-write-up-67a9dc0cae5a][Preparing the data for Transformer pre-training — a write-up | by Steven van ...]]

- general transformer
  + [[http://nlp.seas.harvard.edu/2018/04/03/attention.html][The Annotated Transformer]]
  + [[https://becominghuman.ai/attention-is-all-you-need-16bf481d8b5c][Attention is all you need. An explanation about transformer | by Pierrick RUG...]]
  + [[http://vandergoten.ai/2018-09-18-attention-is-all-you-need/][Attention Is All You Need]]


* steps
** TODO project proposal
DEADLINE: <2021-02-05 Fri>

- file:./tex/proposaltemplate.tex
- file:./tex/proposalbibliography.bib

*** DONE understand paper: Translation byu jointly learning to align and translate
*** DONE understand transformer
**** DONE understand multi-headed attention
*** TODO understand BERT
*** TODO read on more efficient transformers
*** TODO find tweaks to BERT possible
*** TODO narrow down list of papers to reference in proposal

** TODO understand code of transformer
** TODO understand how to use hugging face transformers library
** TODO understand (fine-tunin) training for question answering
** TODO implement albert for Squad
** TODO read on more advanced topics like meta learning


* looking further for lit review
** attention concept
*** Attention in Natural Language Processing
**** intro
Four dimensions to the taxonomy:
- the representation of the input,
- the compatibility function,
- the distribution function,
- the multiplicity of the input and/or output.

**** attention models
#+attr_org: :width 700
[[./tex/illustrations/coreAttentionModel.png]]

#+attr_org: :width 800
[[./tex/illustrations/generaelAttentionModel.png]]

**** use of attention
#+begin_quote
Attention enables us to estimate the relevance of the input
elements as well as to combine said elements into a com-
pact representation—the context vector—that condenses the
characteristics of the most relevant elements. Because the
context vector is smaller than the original input, it requires
fewer computational resources to be processed at later stages,
yielding a computational gain.
#+end_quote

#+begin_quote
When the generation of a text sequence is required, as in
machine translation, attention enables us to make use of a
dynamic representation of the input sequence, whereby the
whole input does not have to be encoded into a single vector.
At each time step, the encoding is tailored according to the
task, and in particular, q represents an embedding of the
previous state of the decoder. More generally, the possibility to
perform attention with respect to a query q allows us to create
representations of the input that depend on the task context,
creating specialized embeddings. This is particularly useful in
tasks, such as sentiment analysis and information extraction.
#+end_quote

#+begin_quote
Since attention can create contextual representations of an
element, it can also be used to build sequence-to-sequence
annotators, without resorting to RNNs or convolutional neural
networks (CNNs), as suggested by Vaswani et al. [36],
who rely on an attention mechanism to obtain a whole
encoder/decoder architecture.
#+end_quote

#+begin_quote
Attention can also be used as a tool for selecting specific
words. This could be the case, for example, in dependence
parsing [97] and in cloze question-answering tasks [66], [70].
In the former case, attention can be applied to a sentence in
order to predict dependences. In the latter, attention can be
applied to a textual document or to a vocabulary to perform a
classification among the words.
#+end_quote

#+begin_quote
Finally, attention can come in handy when multiple inter-
acting input sequences have to be considered in combination.
In tasks such as question answering, where the input consists
of two textual sequences—for instance, the question and
the document or the question and the possible answers—an
input encoding can be obtained by considering the mutual
interactions between the elements of such sequences, rather
than by applying a more rigid a priori defined model.
#+end_quote

**** taxonomy for attention models
#+begin_quote
In NLP-related tasks, generally, K and V are representations
of parts of documents, such as sequences of characters, words,
or sentences. These components are usually embedded into
continuous vector representations and then processed through
key/value annotation functions (called kaf and vaf in Fig. 4),
so as to obtain a hidden representation resulting in K and V .
Typical annotation functions are RNN layers such as gated
recurrent units (GRUs), long short-term memory networks
(LSTMs), and CNNs. In this way, k i and v i represent an input
element relative to its local context.
#+end_quote

#+begin_quote
We made a distinction between two input sources: the input sequence, represented
by K and V , and the query, represented by q. However, some architectures
compute attention only based on the input sequence. These architectures are
known as self-attentive or intraattentive mod- els.
#+end_quote


#+begin_quote
The commonest one amounts to the application of multiple steps of attention to a
vector K , using the elements k t of the same vector as query at each step [18],
[36]. At each step, the weights a i t represent the relevance of k i with
respect to k t , yielding d K separate context embeddings, c t , one per key.
#+end_quote

#+begin_quote
Attention could thus be used as a sequence-to-sequence model, as an alternative
to CNNs or RNNs (see Fig. 5). In this way, each element of the new sequence may
be influenced by elements of the whole input, incorporating contextual
information without any locality boundaries. This is especially interesting
since it could overcome a well-known shortcoming of RNNs: their limited ability
of modeling long-range dependences [140]. For each element k t , the resulting
distribution of the weights a t should give more emphasis to words that strongly
relate to k t . The analysis of these distributions will, therefore, provide
information regarding the relationship between the elements inside the sequence.
Modern text-sequence generation systems often rely on this approach
#+end_quote

**** hiarchical attention
#+begin_quote
Hierarchical-Input Architectures: In some tasks, portions
of input data can be meaningfully grouped together into higher
level structures, where hierarchical-input attention models can
be exploited to subsequently apply multiple attention modules
at different levels of the composition, as shown in Fig. 6.
Consider, for instance, data naturally associated with a
two-level semantic structure, such as characters (the “micro”
elements) forming words (the “macro” elements) or words
forming sentences. Attention can be first applied to the rep-
resentations of micro elements k i , so as to build aggregate
representations k j of the macro elements, such as context
vectors. Attention could then be applied again to the sequence
of macroelement embeddings, in order to compute an embed-
ding for the whole document D. With this model, attention
first highlights the most relevant micro elements within each
macro element and, then, the most relevant macro elements in
the document. For instance, Yang et al. [52] applied attention
first at word level, for each sentence in turn, to compute
sentence embeddings. Then, they applied attention again on
the sentence embeddings to obtain a document representation.
7
With reference to the model introduced in Section II, embed-
dings are computed for each sentence in D, and then, all
such embeddings are used together as keys K to compute the
document-level weights a and eventually D’s context vector c.
The hierarchy can be extended further. For instance, Wu et al.
[141] added another layer on top, applying attention also at
the document level.
If representations for both micro- and macro-level elements
are available, one can compute attention on one level and
then exploit the result as a key or query to compute atten-
tion on the other, yielding two different microrepresenta-
tion/macrorepresentation of D. In this way, attention enables
us to identify the most relevant elements for the task at both
levels. The attention-via-attention model by Zhao and Zhang
[43] defines a hierarchy with characters at the micro level and
words at the macro level. Both characters and words act as
keys. Attention is first computed on word embeddings K W ,
thus obtaining a document representation in the form of a
context vector c W , which in turn acts as a query q to guide the
application of character-level attention to the keys (character
embeddings) K C , yielding a context vector c for D.
Ma et al. [113] identified a single “target” macro-object T
as a set of words, which do not necessarily have to form a
sequence in D, and then used such a macro-object as keys,
K T . The context vector c T produced by a first application of
the attention mechanism on K T is then used as query q in
a second application of the attention mechanism over D, with
the keys being the document’s word embeddings K W .
#+end_quote

**** ref
- [[https://arxiv.org/abs/1902.02181][{1902.02181} Attention in Natural Language Processing]]
- file:~/udem/nlp/project/papers/1902.02181.pdf
#+begin_src tex
@article{DBLP:journals/corr/abs-1902-02181,
  author    = {Andrea Galassi and
               Marco Lippi and
               Paolo Torroni},
  title     = {Attention, please! {A} Critical Review of Neural Attention Models
               in Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1902.02181},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.02181},
  archivePrefix = {arXiv},
  eprint    = {1902.02181},
  timestamp = {Wed, 25 Sep 2019 17:52:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-02181.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
#+end_src

*** attention? attention!
*Note: difficult for now might point to fancy improvements to consider in the future*
**** different mechanisms
#+name: A family of attention mechanisms
#+ATTR_ORG: :width 700
[[./tex/illustrations/afamilyofattentionmechanisms.png]]

**** self attention
#+begin_quote
Self-attention, also known as intra-attention, is an attention mechanism
relating different positions of a single sequence in order to compute a
representation of the same sequence. It has been shown to be very useful in
machine reading, abstractive summarization, or image description generation.
#+end_quote

#+begin_quote
In the show, attend and tell paper, attention mechanism is applied to images to
generate captions. The image is first encoded by a CNN to extract features. Then
a LSTM decoder consumes the convolution features to produce descriptive words
one by one, where the weights are learned through attention. The visualization
of the attention weights clearly demonstrates which regions of the image the
model is paying attention to so as to output a certain word.
#+end_quote
show and tell paper: [[https://arxiv.org/abs/1502.03044][{1502.03044} Show, Attend and Tell: Neural Image Caption Generation with Visu...]]

- Soft Attention: the alignment weights are learned and placed “softly” over all
  patches in the source image; essentially the same type of attention as in
  Bahdanau et al., 2015.

  + Pro: the model is smooth and differentiable.
  + Con: expensive when the source input is large.

- Hard Attention: only selects one patch of the image to attend to at a time.

  + Pro: less calculation at the inference time.
  + Con: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (Luong, et al., 2015)

**** global vs local attention
#+name: global vs local attention
#+attr_org: :width 700
[[./tex/illustrations/globalvslocalattention.png]]

reference: [[https://arxiv.org/abs/1508.04025][{1508.04025} Effective Approaches to Attention-based Neural Machine Translation]]
**** multi-head self attention: key value query
*important*
#+begin_quote
The transformer views the encoded representation of the input as a
set of key-value pairs, (K,V), both of dimension n (input sequence length);

in the context of NMT, both the keys and values are the encoder hidden states.
In the decoder, the previous output is compressed into a query (Q of
dimension m) and the next output is produced by mapping this query and the set
of keys and values.
#+end_quote

#+attr_org: :width 700
[[./tex/illustrations/transformerscaleddotproduct.png]]

#+attr_org: :width 700
[[./tex/illustrations/multiheadselfattention.png]]
reference: transformer paper

*important*
#+begin_quote
Rather than only computing the attention once, the multi-head mechanism runs
through the scaled dot-product attention multiple times in parallel.

The independent attention outputs are simply concatenated and linearly transformed
into the expected dimensions.

According to the paper, “multi-head attention allows the model
to jointly attend to information from different representation subspaces at
different positions. With a single attention head, averaging inhibits this.”
#+end_quote

**** TODO snail
**** ref
- [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
#+begin_src tex
@article{weng2018attention,
  title   = "Attention? Attention!",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io/lil-log",
  year    = "2018",
  url     = "http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html"
}
#+end_src


* ressources
** arxiv links
- [[https://arxiv.org/abs/2009.06732][arxiv Efficient transformers a survey]]
- [[https://arxiv.org/abs/1807.03819][arxiv universal transformers]]
- [[https://arxiv.org/abs/2006.15595][{2006.15595} Rethinking Positional Encoding in Language Pre-training]]

** non academic links

- easier explanations of transformer
  + [[https://jalammar.github.io/illustrated-transformer/][The Illustrated Transformer – Jay Alammar – Visualizing machine learning one ...]]

- Transformer (with code)
  + [[http://nlp.seas.harvard.edu/2018/04/03/attention.html][The Annotated Transformer]]
  + [[https://becominghuman.ai/attention-is-all-you-need-16bf481d8b5c][Attention is all you need. An explanation about transformer | by Pierrick RUG...]]
  + [[http://vandergoten.ai/2018-09-18-attention-is-all-you-need/][Attention Is All You Need]]

- attention
  + [[https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html][Attention? Attention!]]
  + [[https://theaisummer.com/attention/][How Attention works in Deep Learning: understanding the attention mechanism i...]]

- positional encoding
  + [[https://kazemnejad.com/blog/transformer_architecture_positional_encoding/][Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's ...]]
  + [[https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model][nlp - What is the positional encoding in the transformer model? - Data Scienc...]]

- machine translation through alignment
  + [[https://www.tensorflow.org/tutorials/text/nmt_with_attention][Neural machine translation with attention  |  TensorFlow Core]]

- videos
  + [[https://www.youtube.com/watch?v=rBCqOTEfxvg][Attention is all you need; Attentional Neural Network Models | Łukasz Kaiser ...]]
  + [[https://www.youtube.com/watch?v=-QH8fRhqFHM][The Narrated Transformer Language Model - YouTube]]
  + [[https://www.youtube.com/watch?v=S27pHKBEp30][LSTM is dead. Long Live Transformers! - YouTube]]

** github links
- [[https://github.com/google-research/bert][GitHub - google-research/bert: TensorFlow code and pre-trained models for BERT]]
- [[https://github.com/huggingface/transformers][GitHub - huggingface/transformers: Transformers: State-of-the-art NLP]]

** misc
- [[https://www.overleaf.com/3324588169zyyzyysrrtmw][Overleaf, Online LaTeX Editor]]
- [[https://rajpurkar.github.io/SQuAD-explorer/][The Stanford Question Answering Dataset]]
- [[https://www.overleaf.com/learn/latex/bibliography_management_with_bibtex][Bibliography management with bibtex - Overleaf, Online LaTeX Editor]]
- [[https://www.overleaf.com/learn/latex/Inserting_Images][Inserting Images - Overleaf, Online LaTeX Editor]]
- [[https://github.com/hlissner/doom-emacs][GitHub - hlissner/doom-emacs: An Emacs framework for the stubborn martian hacker]]
- [[https://orgmode.org/worg/org-tutorials/][Org tutorials]]
